%!TEX root = ../main.tex

\chapter{Implementierung} 

\label{Implementierung}

% Docs for algorithms: 
% http://mirror.physik-pool.tu-berlin.de/pub/CTAN/macros/latex/contrib/algorithmicx/algorithmicx.pdf

% Quelle für Gradienten: TODO: remove
% http://jlmelville.github.io/sneer/gradients.html#hellinger_distance
% https://jlmelville.github.io/smallvis/theory.html#umap

%----------------------------------------------------------------------------------------

% \newcommand{\indexij}[1][v]{{#1}_{ij}}  % Symbol for v_{ij}
\newcommand{\vij}{v_{ij}}  % Symbol for b_{ij}
\newcommand{\wij}{w_{ij}}  % Symbol for w_{ij}
\newcommand{\deriv}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\sumtup}[2]{\sum_{{#1},{#2} = 1}^N}
\newcommand{\sumtups}[1]{\sum_{{#1} = 1}^N}

%----------------------------------------------------------------------------------------

% N O T I Z E N
% Cite Numba 24

% %\begin{equation}
% \begin{align*}
%    &\qquad&
%    \sum_{i=1}^n \exp\left(\frac{-(\text{knn-dists}_i - \rho)}{\sigma}\right) &= \log_2(n) \\
%    \iff&& \sum_{i=1}^n (\exp(-(\text{knn-dists}_i - \rho)) - \exp(\sigma)) &= \log_2(n) \\
%    \iff&& \left(\sum_{i=1}^n \exp(-(\text{knn-dists}_i - \rho))\right) - n\exp(\sigma) &= \log_2(n) \\
%    \iff&& \sum_{i=1}^n \exp(-(\text{knn-dists}_i - \rho)) &= \log_2(n) + n\exp(\sigma) \\
%    \iff&& \log_e \left(\frac{\sum_{i=1}^n \exp(-(\text{knn-dists}_i - \rho)) - \log_2(n)}{n}\right) &= \sigma \\
% \end{align*}
% %\end{equation}

% T E X T B A U S T E I N E
% 

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

% \section{Einleitung}
   In diesem Kapitel möchten wir die Implementierung des UMAP Verfahren beschreiben. 
   Die vollständige Berechnung aller Simplizes hat eine exponentielle Laufzeit, da hierfür 
   alle $2^N$ Teilmengen unseres $N$-elementigen Datensatzes betrachtet werden müssten. 
   In aktuellen Implementierungen werden hingegen nur alle 
   zweielementigen Teilmengen betrachtet. 
   Wie wir in Kapitel \ref{Experimente} sehen werden, liefert uns diese Approximation des 
   \cech-Komplexes sehr gute visuelle Ergebnisse.  % TODO: Cech Komplex?
   In Kapitel \ref{Zusammenfassung} werden wir Ansätze erwähnen um Simplizes höherer 
   Ordnungen effizient zu finden. 

   Zunächst werden wir die Notation aus Kapitel \ref{UMAP} anpassen, da wir nur das 1-Skelett der 
   topologischen Repräsentationen betrachten. Danach werden wir die Zielfunktion explizit 
   angeben und den Gradienten herleiten. 
   In Abschnitt \ref{seq:pseudo} werden wir den UMAP Algorithmus im Pseudo-Code angeben.
   Die darauf folgenden Abschnitte identifizieren rechenintensive Schritte der von uns 
   verwendeten Implementierung \cite{cpu} und beschreiben Alternativen.
   Abschnitt \ref{seq:hyper} gibt eine Beschreibung der Hyperparameter an. 

%----------------------------------------------------------------------------------------

\section{Numerische Formulierung der Optimierung} \label{seq:numerische_formulierung}
   Um eine Unterscheidung zwischen der theoretischen Sichtweise auf das UMAP Verfahren, welche alle $k$-Simplizes 
   $(1 \leq k \leq N)$ berücksichtigt, und der praktischen Implementierung zu verdeutlichen, werden wir die 
   verwendete Notation anpassen. 

   Die unsicheren Mengen $(A, \mu), (A, \nu)$ aus Gleichung (\ref{eq:crossentropy}) lassen sich als gewichtete Graphen, 
   in Form einer Adjazenzmatrix darstellen. % TODO: Für den Fall, dass nur 1-Simplizes betrachtet werden
   Das 1-Skelett der hochdimensionalen Repräsentation notieren wir als Adjazenzmatrix $V$ mit $\vij = \mu(a)$, für 
   ein 1-Simplex $a$, mit Facetten $i$ und $j$. 
   % TODO: Wahl von \vij

   Die Wahl des Zugehörigkeitsgrades für 1-Simplizes der niedrigdimensionalen Repräsentation der Daten ist wie folgt gegeben:

   \begin{equation}
      \wij = (1+a(\norm{\mathbf{y}_i - \mathbf{y}_j}^2_2)^b)^{-1}, \label{eq:wij} % TODO: wie sind a, b gewählt?
   \end{equation}

   % TODO: für a=b=1 tSNE, b=1 LargeVis

   wobei $\mathbf{y}_i, 1 \leq i \leq N$ die Vektoren der Einbettung sind. Die Wahl der Werte $a,b$ werden wir in 
   Abschnitt \ref{seq:hyper} beschreiben.


   Somit erhalten wir folgende Kreuzentropie zwischen den Graphen der hoch- und niedrigdimensionalen 
   Darstellung der Daten:

   \begin{alignat}{2}
      C(V, W) &= \sumtup{i}{j} \vij \log\left(\frac{\vij}{\wij}\right) + (1-\vij) \log\left(\frac{1-\vij}{1-\wij}\right) \\
              &= \begin{aligned}[t]
                     &  \sumtup{i}{j} (\vij \log(\vij) + (1-\vij) \log(1-\vij)) \\
                     &- \sumtup{i}{j} (\vij \log(\wij)) - \sumtup{i}{j} ((1-\vij) \log(1-\wij))
                  \end{aligned} \\
              &=  C_v - \sumtup{i}{j} (\vij \log(\wij) + (1-\vij) \log(1-\wij)) \label{eq:loss}
   \end{alignat}

   Der Term $C_v$ bleibt während der Minimierung der Funktion bezüglich der $\mathbf{y}_i$ konstant.
   
   Aufgrund der Wahl einer differenzierbaren Funktion für den Zugehörigkeitsgrad der 1-Simplizes, 
   bzw. der Gewichte des niedrigdimensionalen Graphen, lässt sich nun der Gradient der Zielfunktion (\ref{eq:loss}) herleiten.

   Mittels der Kettenregel ergibt sich mit $d_{ij} \coloneqq \norm{\mathbf{y}_i - \mathbf{y}_j}_2$:

   \begin{align}
   \quad
      \deriv{C}{\mathbf{y}_i} &= \sumtup{k}{l} \deriv{C}{w_{kl}} \sumtup{m}{n} \deriv{w_{kl}}{d^2_{mn}} \sumtup{p}{q} \deriv{d^2_{mn}}{d_{pq}} \deriv{d_{pq}}{\mathbf{y}_i} \\
                     &= \sumtup{k}{l} \deriv{C}{w_{kl}} \sumtup{m}{n} \deriv{w_{kl}}{d^2_{mn}} \deriv{d^2_{mn}}{d_{mn}} \deriv{d_{mn}}{\mathbf{y}_i} \\
                     &= 2 \sumtup{k}{l} \deriv{C}{w_{kl}} \sumtups{m} \deriv{w_{kl}}{d^2_{mi}} \deriv{d^2_{mi}}{d_{mi}} \deriv{d_{mi}}{\mathbf{y}_i} \\
                     &= 2 \sumtups{m} \left( \sumtup{k}{l} \deriv{C}{w_{kl}} \deriv{w_{kl}}{d^2_{mi}} \right) \deriv{d^2_{mi}}{d_{mi}} \deriv{d_{mi}}{\mathbf{y}_i}  \label{eq:deriv-2}\\
                     &= 4 \sumtups{m} \left( \sumtup{k}{l} \deriv{C}{w_{kl}} \deriv{w_{kl}}{d^2_{mi}} \right) (\mathbf{y}_i - \mathbf{y}_m) \label{eq:deriv-1}
   \end{align}

   Bei der Umformung von (\ref{eq:deriv-2}) nach (\ref{eq:deriv-1}) haben wir verwendet, dass $d_{mi}$ die euklidische Norm ist. 
   Für Einbettungen in andere metrische Räume müsste der Gradient an dieser Stelle entsprechend angepasst werden. Da wir 
   das UMAP Verfahren im wesentlichen zur Visualisierung im $\R^2$ benutzen ist die Wahl der euklidischen Norm gerechtfertigt.
   Die übrigen Umformungen ergeben sich aus umordnen und wegfallen der Terme. 

   (\ref{eq:deriv-1}) lässt sich weiter umformen, dazu nutzen wir:

   \begin{align}
      \deriv{C}{\wij} &= - \frac{\vij}{\wij} + \frac{1-\vij}{1-\wij} = \frac{\wij - \vij}{\wij (1 - \wij)} \\
      \deriv{\wij}{d^2_{ij}} &= -bad_{ij}^{2(b-1)} \wij^2 = - \frac{b}{d_{ij}^2} \wij (1 - \wij)
   \end{align}

   Der UMAP Gradient ist also gegeben durch:

   \begin{equation}
      \deriv{C}{\mathbf{y}_i} = 4 \sumtups{j} (\wij - \vij) \frac{b}{d_{ij}^2} (\mathbf{y}_i - \mathbf{y}_j)
      \label{eq:umapgrad}
   \end{equation}

%----------------------------------------------------------------------------------------

\section{Pseudo-Code} \label{seq:pseudo}
   Nun können wir das UMAP Verfahren für die 1-Skelette der topologischen Repräsentation angeben.

   \begin{algorithm} %TODO: Verändere x zu x_i (y zu y_j) um Notation konsistent zu halten
   \caption{UMAP Algorithmus}
   \label{algorithm:umap}
   \begin{algorithmic}[1]
   \Function{UMAP}{$X, N, D, d, min\_dist, n\_epochs$}  %description
      \For{$x \in X$}
         \State $knn(x) \gets k\text{-nächste-Nachbarn}(x)$ \label{alg:kNN}
         \State $graph(x) \gets \bot_{y \in knn(x)} (\{x, y\}, exp(-d_{x,y})) \ \bot \ \bot_{y \in X \setminus \{x\}} (\{x, y\}, 0)$ \label{alg:graph}
      \EndFor
      \State $V \gets \text{gewichtete Adjazenzmatrix}(\bigcup_{x \in X} graph(x))$ \label{alg:adj}
      \State $D \gets \text{Grad-Matrix des Graphen } V$ \label{alg:grad}
      \State $L \gets D^{1/2} (D-V) D^{1/2}$ \Comment{Symmetrische normalisierte Laplace-Matrix}
      \State $evec \gets \text{sortierte Eigenvektoren von } L$
      \State $Y \gets evec[1,\dots,d\text{+}1]$ \label{alg:spec}
      \State $Y \gets OptimiereEinbettung(Y, min\_dist, n\_epochs)$
      \State \textbf{return} $Y$
   \EndFunction
   \end{algorithmic}
   \end{algorithm}

   In Abschnitt \ref{seq:kNN} werden zwei effiziente Verfahren für die k-nächste-Nachbarn Suche (Zeile \ref{alg:kNN}) angeben. 
   
   %TODO: wahl von d_xy

   Der in Zeile \ref{alg:graph} verwendete $\bot$ Operator verdeutlicht, dass wir die Kantengewichte mittels wahrscheinlichkeitstheoretischer 
   t-Conorm vereinigen. % TODO: Referenzen t-Conorm einfügen
   In den von uns verwendeten Implementierung des UMAP Verfahrens \cite{cpu,GPUUMAP} wird ein zusätzlicher Hyperparameter verwendet, welcher 
   eine Verallgemeinerung der Vereinigung darstellt (siehe: \code{set\_op\_mix\_ratio} in Abschnitt \ref{seq:hyper}). %TODO: Verweis auf set_op_mix_ratio
   Zusätzlich ist zu beachten, dass die Vereinigung in Zeile \ref{alg:graph} ungerichtete Kanten betrachtet. Der so erhaltene ungerichtete Graph 
   besitzt aufgrund der Symmetrie der t-Conorm wohldefinierte Kantengewichte.

   Die Grad-Matrix $D$ (Zeile \ref{alg:grad}) ist eine Diagonalmatrix, wobei  $d_{ii} \coloneqq  \sum_{1 \leq k \leq N} v_{ik}, (1 \leq i \leq N)$. 
   Für den Spezialfall, der ungewichteten Adjazenzmatrix, beschreibt $d_{ii}$ also den Grad des Knoten $i$. 

   Die Initialisierung der niedrigdimensionalen Repräsentation $Y$ in Zeile \ref{alg:spec} ist die spektrale Einbettung des Graphen. 
   Eine genauere Beschreibung warum die so gewählte Initialisierung sinnvoll ist findet sich in Abschnitt \ref{seq:spec}. Dabei ist 
   zu beachten, dass eine Lösung des Eigenwertproblems nur von der Größe der Einbettungsdimension abhängig ist. Da wird stets $d \ll D$ 
   betrachten ist eine effiziente Implementierung mittels sukzessiver Eigenwertsuche möglich. % TODO: Referenz für sukkzesive suche % TODO: Verschieben in section unten.

%----------------------------------------------------------------------------------------

\section{Spektrale Einbettung} \label{seq:spec}
   Der Laplace Operator ist eine diskrete Approximation des Laplace-Beltrami Operators. Spektrale Einbettung \dots
   % TODO: ergänzen

%----------------------------------------------------------------------------------------

\section{Profiling}
   In Kapitel \ref{Experimente} werden wir genauer auf die tatsächliche Laufzeit des UMAP Algorithmus eingehen. 
   An dieser Stelle möchten wir die rechenintensiven Subroutinen des Verfahrens ausmachen. Dafür haben wir den 
   Python cProfiler verwendet, dieser misst die Laufzeit der aufgerufenen Funktionen. 
   Um für verschiedene Umgebungsdimensionen vergleichbare Ergebnisse zu erhalten, haben wir $N = \num{10000}$ 
   Datenpunkte in $10$ unterschiedlichen $D = [100, 500, 1000, 5000, 10000, 50000]$-dimensionalen Gauß-verteilten 
   Datenwolken gewählt. Diese Daten wurden dann in den zweidimensionalen Raum eingebettet.
   
   Dabei ist uns aufgefallen, dass besonders der k-nächste-Nachbarn-Algorithmus und die Optimierung mittels 
   stochastischem Gradienten Verfahren einen großen Teil der Laufzeit des Verfahrens beanspruchen. In Tabelle 
   \ref{table:profiling} sind die Ergebnisse der Profilierung zusammengefasst. 
   Insbesondere scheint die k-nächste-Nachbarn Suche die Laufzeit des UMAP Verfahren für 
   hochdimensionale Daten stark zu beeinflussen. 
   Wir werden beide rechenintensiven Subroutinen im folgenden betrachten und geeignete Verbesserungen diskutieren.

   \begin{table}
   \centering
   \begin{tabular}{l|ll}
   D     & Laufzeit NN-Descent & Laufzeit der Optimierung \\ \hline
   100   & 9\%                 & 75,3\%                   \\
   500   & 12\%                & 73,8\%                   \\
   1000  & 14\%                & 72,9\%                   \\
   5000  & 30,4\%              & 58\%                     \\
   10000 & 44\%                & 45,1\%                   \\
   50000 & 78,8\%              & 14,8\%                  
   \end{tabular}
   \caption{$D$ beschreibt die Größe der Umgebungsdimension. Abhängig von D haben wir die Laufzeit des UMAP Verfahrens 
            profiliert. Die zweite und dritte Spalte beziehen sich auf die relativen Laufzeiten des kNN Verfahrens und 
            der Optimierung der Einbettung.}
   \label{table:profiling}
   \end{table} 

%----------------------------------------------------------------------------------------

% N O T I Z E N
% sub-sampling bei Wortrepräsentationen sorgt für verbesserte Laufzeiten (2x - 10x)
% und Genauigkeit weniger häufig repräsentierter Wörter nimmt zu

% Negative sampling
% Noise Contrastive Estimation (NCE), Gutmann and Hyvarinen
% Ein Gutes Modell, kann Daten und Rauschen mittels logistischer Regression unterscheiden
% Benötigt samples und numerische Wahrscheinlichkeiten der Raschverteilung 
% Neg. Sampling benötigt nur samples
% Für die Wahrsscheinlichkeitsverteilung wird in der Word2Vec Version die Unigram Verteilung ^3/4 gewählt


\section{Gradientenverfahren} \label{seq:SGD}
   Um die Zielfunktion (\ref{eq:loss}) zu optimieren bietet sich die Wahl eines Gradientenverfahrens an, da eine differenzierbare 
   Approximation des Zugehörigkeitsgrades (siehe Gleichung \ref{eq:wij}) gegeben ist.

   In den vergangenen Jahren gab es viele Weiterentwicklungen, insbesondere bezüglich der Konvergenzgeschwindigkeit, 
   von Gradientenverfahren. Diese werden unter anderem für das trainieren neuronaler Netzwerke bei der Backpropgation genutzt. 
   In \cite{SGDGPU} werden verschiedene Implementierungen verglichen.

   Das UMAP Verfahren nutzt für die Optimierung der Zielfunktion die \textit{negative sampling} Methode aus dem Bereich 
   der Worteinbettungen \cite{Mikolov}. Wir werden diese Methode vorstellen und dann Algorithmus \ref{algorithm:umap}, 
   um die Subroutine \textsc{OptimiereEinbettung} vervollständigen.
   


   % Optimiert Suchrichtung des Gradienten da wichtige Beispiele betrachtet werden. 
   \subsection*{Sub-sampling}
      siehe Word2Vec

   \subsection*{Optimierung des UMAP Gradienten}
      Pseudo Code der Optimierung
      \begin{algorithm}
      \caption{OptimiereEinbettung}
      \label{algorithm:optimize}
      \begin{algorithmic}[1]
      \Function{OptimiereEinbettung}{$Y, V, W, n\_epochs$}  %description
         \For{$x \in X$}
            \State $knn(x) \gets k\text{-nächste-Nachbarn}(x)$ 
            \State $graph(x) \gets \bot_{y \in knn(x)} (\{x, y\}, exp(-d_{x,y})) \ \bot \ \bot_{y \in X \setminus \{x\}} (\{x, y\}, 0)$
         \EndFor
         \State $V \gets \text{gewichtete Adjazenzmatrix}(\bigcup_{x \in X} graph(x))$
         \State $D \gets \text{Grad-Matrix des Graphen } V$
         \State $L \gets D^{1/2} (D-V) D^{1/2}$ \Comment{Symmetrische normalisierte Laplace-Matrix}
         \State $evec \gets \text{sortierte Eigenvektoren von } L$
         \State $Y \gets evec[1,\dots,d\text{+}1]$
         \State $Y \gets OptimiereEinbettung(Y, min\_dist, n\_epochs)$
         \State \textbf{return} $Y$
      \EndFunction
      \end{algorithmic}
      \end{algorithm}

%----------------------------------------------------------------------------------------

\section{Nächste-Nachbarn-Klassifikation} \label{seq:kNN}
% Einer der beiden rechenintensivsten Subroutinen im UMAP Algorithmus ist die nächste Nachbar suche.
% in \cite{Tang} (k-NN is bottleneck) werden 3 verschiedene Methoden verglichen. Sehr effizient ist auch die 
% Facebook FAISS Methode.
   Zum effizienten finden der 1-Simplizes der topologischen Repräsentation unserer Daten, benötigen wir einen 
   k-nächste-Nachbarn-Algorithmus (kurz: \textit{kNN-Algorithmus}). 
   %TODO: Beschreibung was ein kNN-Alg macht

   Das Ergebnis eines kNN-Algorithmus wird meist in einem ungerichteten Graph -- dem kNN-Graph -- dargestellt, 
   wobei die Knoten den Datenpunkten entsprechen und die Kanten den Nachbarschaftsbeziehungen, 
   somit besitzt jeder Knoten Grad k.

   Bei einer naiven Implementierung beträgt die Laufzeit $\mathcal{O}(N^2D)$ (wobei N die Anzahl der Datenpunkte  % TODO: warum N^2D?
   und D die Dimension der Datenpunkte ist). Mit einer effizienten Implementierung ist in der Praxis eine 
   annähernd in N lineare Laufzeit möglich. Die Herangehensweisen lassen sich nach \cite{Tang} in drei Kategorien 
   einteilen. (1) Baum basierte Verfahren auf Partitionen des Raumes, (2) Hashfunktionen auf lokalen Teilgebieten des Raumes 
   (3) Nachbarschafts-Erkundungen. 

   Wir möchten nun zwei Verfahren vorstellen. 

%-----------------------------------

   \subsection*{NN-Descent}
      Der NN-Descent Algorithmus \cite{k-NNG} beruht auf dem Prinzip der Nachbarschafts-Erkundungen. Dabei wird ein 
      initialer kNN-Graph iterativ verbessert, unter der Annahme, dass die Nachbarschaftsbeziehung 
      transitiv ist, für zwei vorhandene Nachbarschaftspaare $(x, y), (y,z)$ also mit hoher Wahrscheinlichkeit auch 
      ein Nachbarschaftspaar $(x,z)$ im kNN-Graph existiert. 
      Der initiale Graph im NN-Descent Verfahren wird dabei zufällig gewählt. Dies kann dazu führen, dass nur 
      lokal optimale k-NN-Graphen gefunden werden. Dies könnte laut \cite{EFANNA} dadurch verbessert werden, indem 
      für die Initialisierung \enquote{random projection trees}, wie in \cite{Tang}, verwendet werden. 

      Ein Vorteil des NN-Descent Verfahren ist, dass kein globaler Index der verwaltet werden muss. Somit ist eine 
      Anwendung auf großen Datensätzen möglich welche nicht komplett in den Arbeitsspeicher (RAM) des verwendeten Rechners 
      geladen werden können. 

      Nachteil des NN-Descent Algorithmus ist die Speicherplatzkomplexität, diese ist durch $\mathcal{O}(N^2)$ beschränkt. 
      Im wesentlichen ist dies dadurch begründet, dass paarweise die Ähnlichkeit, welche im Falle von UMAP durch die Metrik 
      des Umgebungsraums gegeben ist, gespeichert wird. Aufgrund dessen, dass nur lokale Optima garantiert sind, ist das Ergebnis 
      des NN-Descent Verfahren approximativ. In \cite{UMAP} wird jedoch argumentiert, dass dies wegen des Informationsverlust 
      bei Dimensionsreduktionen kaum Auswirkungen auf die resultierende Einbettung hat. 

%-----------------------------------

   \subsection*{FAISS}
      Das FAISS Bibliothek \cite{FAISS} nutzt die Architektur einer GPU aus. Dabei baut FAISS eine effiziente Datenstruktur, 
      welche für die Vektoren die nächsten Nachbarn speichert. Somit ist eine sehr schnelle Implementierung 
      für das aufstellen des k-NN-Graphen möglich. 

      Der RAM der meisten GPUs ist stark begrenzt. Um dennoch mit großen Datensätzen zu arbeiten werden komprimierte 
      Darstellungen der Vektoren genutzt. % TODO: Referenzen einfügen, aus FAISS
      Für FAISS werden \enquote{product quantization codes} genutzt. % TODO: referenz und kurze Beschreibung 

      Vorteile der FAISS Datenstruktur sind die effiziente Implementierung auf GPUs und das sowohl exakte Ergebnisse 
      sowie Approximationen für die nächsten Nachbarn angegeben werden können. Die Rückgabe approximativer 
      Ergebnisse erhält Laufzeit sowie Speicherplatz Vorteile. 

      Nachteil des FAISS Verfahren ist, dass zurzeit nur die euklidische Distanz unterstützt wird. 

%----------------------------------------------------------------------------------------

\section{Hyperparameter} \label{seq:hyper}

   \begin{itemize}
      \item \code{n\_neighbors}:
            beschreibender Text
        % The size of local neighborhood (in terms of number of neighboring
        % sample points) used for manifold approximation. Larger values
        % result in more global views of the manifold, while smaller
        % values result in more local data being preserved. In general
        % values should be in the range 2 to 100.
      \item $\code{metric}$
         Text
      \item $\code{n\_epochs}$
         Text
      \item \code{set\_op\_mix\_ratio}
        % Interpolate between (fuzzy) union and intersection as the set operation
        % used to combine local fuzzy simplicial sets to obtain a global fuzzy
        % simplicial sets. Both fuzzy set operations use the product t-norm.
        % The value of this parameter should be between 0.0 and 1.0; a value of
        % 1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy
        % intersection.
      \item $\code{local\_connectivity}$ 
         % The local connectivity required -- i.e. the number of nearest
         % neighbors that should be assumed to be connected at a local level.
         % The higher this value the more connected the manifold becomes
         % locally. In practice this should be not more than the local intrinsic
         % dimension of the manifold.
      \item \code{repulsion\_strength}
         % Weighting applied to negative samples in low dimensional embedding
         % optimization. Values higher than one will result in greater weight
         % being given to negative samples.
      \item \code{a, b}:
            Erwähne wie a und b standardmäßig gewählt werden, zeige Plot von a, b
            Erwähne \code{min\_dist} und \code{spread}
   \end{itemize}

      

