%!TEX root = ../main.tex

\chapter{Implementierung} \label{Implementierung}

%----------------------------------------------------------------------------------------

%Define math commands used in this chapter
   \newcommand{\indexij}[1][v]{{#1}_{ij}}  % Symbol for inp_{ij}
   \newcommand{\vij}{v_{ij}}  % Symbol for b_{ij}
   \newcommand{\wij}{w_{ij}}  % Symbol for w_{ij}
   \newcommand{\deriv}[2]{\frac{\partial {#1}}{\partial {#2}}}
   \newcommand{\sumtup}[2]{\sum_{{#1},{#2} = 1}^N}
   \newcommand{\sumtups}[1]{\sum_{{#1} = 1}^N}
   \newcommand{\Xg}{\mathcal{X}_G}  % Symbol for Graph of \Xrep
   \newcommand{\Xgi}{\mathcal{X}_{{G}_i}}  % Symbol for Graph of \Xrep
   \newcommand{\wX}{w_\mathcal{X}}  % Symbol for Graph of \Xrep
   \newcommand{\WX}{W_\mathcal{X}}  % Symbol for Graph of \Xrep

%----------------------------------------------------------------------------------------

% N O T I Z E N
% Cite Numba 24

% TODO: Bem: Fehler in Kapitel 3 von UMAP wij sollten gewichte der Matrix B sein.

% \begin{align}
%    \sum_{i=1}^n \exp\left(\frac{-(\text{knn-dists}_i - \rho)}{\sigma}\right) = \log_2(n)
% \end{align}

% TODO: Daniel 
% Fallen in 4.9 alle Terme in der Summe weg für kl \neq mi ? Dann würde ich verstehen warum man auf 4.12 kommt. 
% Das würde ich dann aber noch kurz erwähnen. Und entweder in 4.11 oder 4.12 sind die Vorzeichen falsch glaub ich.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

% \section{Einleitung}
   In diesem Kapitel möchten wir die Implementierung des UMAP Verfahren beschreiben. 
   Ideal-typischerweise würden wir dafür die metrischen Räume konstruieren und diese mittels 
   des modifizierten Singulären Mengen Funktors mit der in Gleichung (\ref{eq:toprep}) beschriebenen 
   unscharfen topologischen Repräsentation darstellen. Praktikabel ist dies nicht für große Datensätze 
   umsetzbar, da wir für die Berechnung der unscharfen simplizialen Mengen alle 
   $2^N$ Teilmengen unseres $N$-elementigen Datensatzes betrachten müssten. 

   In aktuellen Implementierungen werden hingegen nur alle zweielementigen Teilmengen betrachtet. 
   Wir werden in Kapitel \ref{Zusammenfassung} Ansätze erwähnen um Simplizes höherer 
   Ordnungen effizient zu finden. 

   Zunächst werden wir in Abschnitt \ref{seq:numerische_formulierung} die Notation anpassen und 
   die Formulierung der Optimierung konkretisieren.
   In Abschnitt \ref{seq:pseudo} werden wir den gesamten UMAP Algorithmus im Pseudo-Code angeben. 
   Wir werden dann mittels Profiling, in Abschnitt \ref{sec:profile}, die rechenintensiven Subroutinen 
   der von uns verwendeten Implementierung \cite{cpu} identifizieren. In den darauffolgenden Abschnitten 
   werden wir die aufwendigen Routinen genauer betrachten, in dem wir in Abschnitt \ref{seq:SGD} 
   das verwendete Verfahren zur Optimierung der Einbettung beschreiben und in Abschnitt \ref{seq:kNN} 
   zwei Verfahren zu Berechnung der $1$-Simplizes diskutieren.
   Abschnitt \ref{seq:hyper} gibt eine Beschreibung der Hyperparameter an. 

%----------------------------------------------------------------------------------------

% TODO: !!!!!!! Kontroliere ob \mu(a) = \vij ist oder = den nicht symetrischen Werten. (dann ändere V und definiere \fij = sym(\vij))
\section{Numerische Formulierung des UMAP Verfahrens} \label{seq:numerische_formulierung}
   Um weitere Überlegungen zu vereinfachen und die praktische Implementierung zu beschreiben, werden wir nun die Notation etwas anpassen.

   Dabei ist zu bemerken, dass wir uns von nun an auf die Konstruktion des $1$-Skelett der unscharfen simplizialen Mengen beschränken. 
   Die Vereinfachung, dass wir keine Simplizes höherer Ordnungen betrachten werden wir in Abschnitt \ref{sec:ausblick} genauer betrachten 
   und Ansätze erwähnen auch Simplizes höherer Ordnungen zu nutzen. 

   Wir werden von nun an die $0$-Simplizes einer (unscharfen) Menge als Knoten, die $1$-Simplizes als Kanten, das $1$-Skelett als Graph 
   und den Zugehörigkeitsgrad einer Kante als Gewicht der Kante bezeichnen. 

   Sei $\Xrep$ die in Gleichung (\ref{eq:toprep}) definierte topologische Repräsentation der Daten aus $X$ und sei $k \in \N$ gegeben. 

   Das $1$-Skelett von $\Xrep$ lässt sich durch einen Graph $\Xg$ beschreiben. Dafür benötigen wir einen Zwischenschritt, in welchem wir 
   $N$ Graphen $\Xgi$ konstruieren, diese beschreiben das Bild $\fSing((X,d_i))$. Dabei sind die Gewichte von $\Xgi$ durch,

   \begin{align}
      w_i(x,y) \coloneqq \begin{cases}
                           \exp \left(\frac{-\max(0, d(x,y)-\rho_i)}{\sigma_i}\right) \quad &\text{,falls } x=\mathbf{x}_i, y=\mathbf{x}_{i_j}\\
                           0 \quad &\text{,sonst}
                         \end{cases}
   \end{align}

   mit $1 \leq j \leq k$, gegeben. $\mathbf{x}_{i_j}$ ist wieder der $j$-ten Nachbar von $\mathbf{x}_i$, $\rho_i$ ist die Distanz von $\mathbf{x}_i$ 
   zum ersten Nachbarn. In Kapitel \ref{UMAP} wurde die Wahl von $\sigma_i$ als Distanz zum $k$-ten Nachbarn zu wählen. 
   In der Praxis werden wir diese Wahl leicht modifizieren, indem wir $\sigma_i$ so wählen, dass folgende Gleichung erfüllt ist: 

   \begin{align}
      \sum_{j=1}^k \exp \left(\frac{-\max(0, d(\mathbf{x}_i,\mathbf{x}_{i_j})-\rho_i)}{\sigma_i}\right) = \log_2(k)
   \end{align}

   % TODO: warum?
   Der gewichtete Graph $\Xgi$ besitzt also die Knotenmenge $V(\Xgi)=X$ und die Kantenmenge $E(\Xgi)=(X \times X)$ mit Kantengewichten $w_i$. 
   Mit $W_i$ bezeichnen wir die zu $\Xgi$ gehörende gewichtete Adjazenzmatrix. 

   Die Vereinigung der $\Xgi$ zu $\Xg$ lässt sich als Anwendung der t-Conorm auf die Gewichte beschreiben. Somit erhalten wir eine neue 
   Gewichtsfunktion $\wX$, mit 

   \begin{align}
      \wX(x,y) \coloneqq w_1(x,y) \perp w_2(x,y) \perp \dots \perp w_N(x,y).
   \end{align}

   Um in Abschnitt \ref{sec:einb} die Wahl der Kreuzentropie zu begründen, haben wir 
   argumentiert, dass man die Zugehörigkeitsfunktionen der Simplizes als Wahrscheinlichkeitsverteilung auf den Simplizes betrachten kann. 
   Diese Argumentation kann man auch anwenden um die Wahl der t-Conorm, aus Beispiel \ref{ex:tNorm}, gegeben durch 
   $\bot(a,b) \coloneqq a + b - ab$, zu begründen. Diese entspricht nämlich genau der Wahrscheinlichkeit der Vereinigung unabhängiger 
   Ereignisse. 
   Nun können wir die Gewichte von $\Xg$ wie folgt angeben, 

   \begin{align}
      \wX(x,y) \coloneqq \text{FEHLT} %\sum_{i=1}^N w_i(x,y) - \sum_{i=2}^N w_1(x,y) \prod_{j=2}^N w_j(x,y).
   \end{align}

   Wir werden nun die $\Yrep$ aus Gleichung (\ref{eq:lowrep}) konstruieren. Dazu müssen wir eine geeignete 

   % Um eine Unterscheidung zwischen der theoretischen Sichtweise auf das UMAP Verfahren, welche alle $k$-Simplizes 
   % $(1 \leq k \leq N)$ berücksichtigt, und der praktischen Implementierung zu verdeutlichen, werden wir die 
   % verwendete Notation anpassen. 

   % Die unsicheren Mengen $(A, \mu), (A, \nu)$ aus Gleichung (\ref{eq:crossentropy}) lassen sich als gewichtete Graphen, 
   % in Form einer Adjazenzmatrix darstellen. % TODO: Für den Fall, dass nur 1-Simplizes betrachtet werden
   % Das 1-Skelett der hochdimensionalen Repräsentation notieren wir als Adjazenzmatrix $V$ mit $\vij = \mu(a)$, für 
   % ein 1-Simplex $a$, mit Facetten $i$ und $j$. 
   % % TODO: Wahl von \vij

   % Die Wahl des Zugehörigkeitsgrades für 1-Simplizes der niedrigdimensionalen Repräsentation der Daten ist wie folgt gegeben:

   % \begin{equation}
   %    \wij = (1+a(\norm{\mathbf{y}_i - \mathbf{y}_j}^2_2)^b)^{-1}, \label{eq:wij} % TODO: wie sind a, b gewählt?
   % \end{equation}

   % % TODO: für a=b=1 tSNE, b=1 LargeVis

   % wobei $\mathbf{y}_i, 1 \leq i \leq N$ die Vektoren der Einbettung sind. Die Wahl der Werte $a,b$ werden wir in 
   % Abschnitt \ref{seq:hyper} beschreiben.


   % Somit erhalten wir folgende Kreuzentropie zwischen den Graphen der hoch- und niedrigdimensionalen 
   % Darstellung der Daten:

   % \begin{alignat}{2}
   %    C(V, W) &= \sumtup{i}{j} \vij \log\left(\frac{\vij}{\wij}\right) + (1-\vij) \log\left(\frac{1-\vij}{1-\wij}\right) \\
   %            &= \begin{aligned}[t]
   %                   &  \sumtup{i}{j} (\vij \log(\vij) + (1-\vij) \log(1-\vij)) \\
   %                   &- \sumtup{i}{j} (\vij \log(\wij)) - \sumtup{i}{j} ((1-\vij) \log(1-\wij))
   %                \end{aligned} \\
   %            &=  C_v - \sumtup{i}{j} (\vij \log(\wij) + (1-\vij) \log(1-\wij)) \label{eq:loss}
   % \end{alignat}

   % Der Term $C_v$ bleibt während der Minimierung der Funktion bezüglich der $\mathbf{y}_i$ konstant.
   
   % Aufgrund der Wahl einer differenzierbaren Funktion für den Zugehörigkeitsgrad der 1-Simplizes, 
   % bzw. der Gewichte des niedrigdimensionalen Graphen, lässt sich nun der Gradient der Zielfunktion (\ref{eq:loss}) herleiten.

   % Mittels der Kettenregel ergibt sich mit $d_{ij} \coloneqq \norm{\mathbf{y}_i - \mathbf{y}_j}_2$:

   % \begin{align}
   % \quad
   %    \deriv{C}{\mathbf{y}_i} &= \sumtup{k}{l} \deriv{C}{w_{kl}} \sumtup{m}{n} \deriv{w_{kl}}{d^2_{mn}} \sumtup{p}{q} \deriv{d^2_{mn}}{d_{pq}} \deriv{d_{pq}}{\mathbf{y}_i} \\
   %                   &= \sumtup{k}{l} \deriv{C}{w_{kl}} \sumtup{m}{n} \deriv{w_{kl}}{d^2_{mn}} \deriv{d^2_{mn}}{d_{mn}} \deriv{d_{mn}}{\mathbf{y}_i} \\
   %                   &= 2 \sumtup{k}{l} \deriv{C}{w_{kl}} \sumtups{m} \deriv{w_{kl}}{d^2_{mi}} \deriv{d^2_{mi}}{d_{mi}} \deriv{d_{mi}}{\mathbf{y}_i} \\
   %                   &= 2 \sumtups{m} \left( \sumtup{k}{l} \deriv{C}{w_{kl}} \deriv{w_{kl}}{d^2_{mi}} \right) \deriv{d^2_{mi}}{d_{mi}} \deriv{d_{mi}}{\mathbf{y}_i}  \label{eq:deriv-2}\\
   %                   &= 4 \sumtups{m} \left( \sumtup{k}{l} \deriv{C}{w_{kl}} \deriv{w_{kl}}{d^2_{mi}} \right) (\mathbf{y}_i - \mathbf{y}_m) \label{eq:deriv-1}
   % \end{align}

   % Bei der Umformung von (\ref{eq:deriv-2}) nach (\ref{eq:deriv-1}) haben wir verwendet, dass $d_{mi}$ die euklidische Norm ist. 
   % Für Einbettungen in andere metrische Räume müsste der Gradient an dieser Stelle entsprechend angepasst werden. Da wir 
   % das UMAP Verfahren im wesentlichen zur Visualisierung im $\R^2$ benutzen ist die Wahl der euklidischen Norm gerechtfertigt.
   % Die übrigen Umformungen ergeben sich aus umordnen und wegfallen der Terme. 

   % (\ref{eq:deriv-1}) lässt sich weiter umformen, dazu nutzen wir:

   % \begin{align}
   %    \deriv{C}{\wij} &= - \frac{\vij}{\wij} + \frac{1-\vij}{1-\wij} = \frac{\wij - \vij}{\wij (1 - \wij)} \\
   %    \deriv{\wij}{d^2_{ij}} &= -bad_{ij}^{2(b-1)} \wij^2 = - \frac{b}{d_{ij}^2} \wij (1 - \wij)
   % \end{align}

   % Der UMAP Gradient ist also gegeben durch:
   % % TODO: Daniel: Fehlt da nicht noch eine innere Summe aus 4.9?
   % \begin{equation}
   %    \deriv{C}{\mathbf{y}_i} = 4 \sumtups{j} (\wij - \vij) \frac{b}{d_{ij}^2} (\mathbf{y}_i - \mathbf{y}_j)
   %    \label{eq:umapgrad}
   % \end{equation}

%----------------------------------------------------------------------------------------

\section{Pseudo-Code} \label{seq:pseudo}
   Nun können wir das UMAP Verfahren für die 1-Skelette der topologischen Repräsentation angeben.

   \begin{algorithm} %TODO: Verändere x zu x_i (y zu y_j) um Notation konsistent zu halten
   \caption{UMAP Algorithmus}
   \label{algorithm:umap}
   \begin{algorithmic}[1]
   \Function{UMAP}{$X, N, D, d, min\_dist, n\_epochs$}  %description
      \For{$x \in X$}
         \State $knn(x) \gets k\text{-nächste-Nachbarn}(x)$ \label{alg:kNN}
         \State $graph(x) \gets \bot_{y \in knn(x)} (\{x, y\}, exp(-d_{x,y})) \ \bot \ \bot_{y \in X \setminus \{x\}} (\{x, y\}, 0)$ \label{alg:graph}
      \EndFor
      \State $V \gets \text{gewichtete Adjazenzmatrix}(\bigcup_{x \in X} graph(x))$ \label{alg:adj}
      \State $D \gets \text{Grad-Matrix des Graphen } V$ \label{alg:grad}
      \State $L \gets D^{1/2} (D-V) D^{1/2}$ \Comment{Symmetrische normalisierte Laplace-Matrix}
      \State $evec \gets \text{sortierte Eigenvektoren von } L$
      \State $Y \gets evec[1,\dots,d\text{+}1]$ \label{alg:spec}
      \State $Y \gets \textsc{OptimiereEinbettung}(Y, min\_dist, n\_epochs)$ \Comment{siehe Alg. \ref{algorithm:optimize}}
      \State \textbf{return} $Y$
   \EndFunction
   \end{algorithmic}
   \end{algorithm}

   In Abschnitt \ref{seq:kNN} werden zwei effiziente Verfahren für die k-nächste-Nachbarn Suche (Zeile \ref{alg:kNN}) angeben. 
   
   %TODO: wahl von d_xy

   Der in Zeile \ref{alg:graph} verwendete $\bot$ Operator verdeutlicht, dass wir die Kantengewichte mittels wahrscheinlichkeitstheoretischer 
   t-Conorm vereinigen. % TODO: Referenzen t-Conorm einfügen
   In den von uns verwendeten Implementierung des UMAP Verfahrens \cite{cpu,GPUUMAP} wird ein zusätzlicher Hyperparameter verwendet, welcher 
   eine Verallgemeinerung der Vereinigung darstellt (siehe: \code{set\_op\_mix\_ratio} in Abschnitt \ref{seq:hyper}). %TODO: Verweis auf set_op_mix_ratio
   Zusätzlich ist zu beachten, dass die Vereinigung in Zeile \ref{alg:graph} ungerichtete Kanten betrachtet. Der so erhaltene ungerichtete Graph 
   besitzt aufgrund der Symmetrie der t-Conorm wohldefinierte Kantengewichte.

   Die Grad-Matrix $D$ (Zeile \ref{alg:grad}) ist eine Diagonalmatrix, wobei  $d_{ii} \coloneqq  \sum_{1 \leq k \leq N} v_{ik}, (1 \leq i \leq N)$. 
   Für den Spezialfall, der ungewichteten Adjazenzmatrix, beschreibt $d_{ii}$ also den Grad des Knoten $i$. 

   % TODO: Spektrale Einbettung beschreiben
   Die Initialisierung der niedrigdimensionalen Repräsentation $Y$ in Zeile \ref{alg:spec} ist die spektrale Einbettung des Graphen. 
   Eine genauere Beschreibung warum die so gewählte Initialisierung sinnvoll ist findet sich in Abschnitt \dots. Dabei ist 
   zu beachten, dass eine Lösung des Eigenwertproblems nur von der Größe der Einbettungsdimension abhängig ist. Da wird stets $d \ll D$ 
   betrachten ist eine effiziente Implementierung mittels sukzessiver Eigenwertsuche möglich. % TODO: Referenz für sukkzesive suche % TODO: Verschieben in section unten.

%----------------------------------------------------------------------------------------

\section{Profiling} \label{sec:profile}
   In Kapitel \ref{Experimente} werden wir genauer auf die tatsächliche Laufzeit des UMAP Algorithmus eingehen. 
   An dieser Stelle möchten wir die rechenintensiven Subroutinen des Verfahrens ausmachen. Dafür haben wir den 
   Python cProfiler verwendet, dieser misst die Laufzeit der aufgerufenen Funktionen. 
   Um für verschiedene Umgebungsdimensionen vergleichbare Ergebnisse zu erhalten, haben wir $N = \num{10000}$ 
   Datenpunkte in $10$ unterschiedlichen $D = [100, 500, 1000, 5000, 10000, 50000]$-dimensionalen Gauß-verteilten 
   Datenwolken gewählt. Diese Daten wurden dann in den zweidimensionalen Raum eingebettet.
   
   Dabei ist uns aufgefallen, dass besonders der k-nächste-Nachbarn-Algorithmus und die Optimierung mittels 
   stochastischem Gradienten Verfahren einen großen Teil der Laufzeit des Verfahrens beanspruchen. In Tabelle 
   \ref{table:profiling} sind die Ergebnisse der Profilierung zusammengefasst. 
   Insbesondere scheint die k-nächste-Nachbarn Suche die Laufzeit des UMAP Verfahren für 
   hochdimensionale Daten stark zu beeinflussen. 
   Wir werden beide rechenintensiven Subroutinen im folgenden betrachten und geeignete Verbesserungen diskutieren.

   \begin{table}
   \centering
   \begin{tabular}{l|ll}
   D     & Laufzeit NN-Descent & Laufzeit der Optimierung \\ \hline
   100   & 9\%                 & 75,3\%                   \\
   500   & 12\%                & 73,8\%                   \\
   1000  & 14\%                & 72,9\%                   \\
   5000  & 30,4\%              & 58\%                     \\
   10000 & 44\%                & 45,1\%                   \\
   50000 & 78,8\%              & 14,8\%                  
   \end{tabular}
   \caption{$D$ beschreibt die Größe der Umgebungsdimension. Abhängig von D haben wir die Laufzeit des UMAP Verfahrens 
            profiliert. Die zweite und dritte Spalte beziehen sich auf die relativen Laufzeiten des kNN Verfahrens und 
            der Optimierung der Einbettung.}
   \label{table:profiling}
   \end{table} 

%----------------------------------------------------------------------------------------

% N O T I Z E N
% sub-sampling bei Wortrepräsentationen sorgt für verbesserte Laufzeiten (2x - 10x)
% und Genauigkeit weniger häufig repräsentierter Wörter nimmt zu

% Negative sampling
% Noise Contrastive Estimation (NCE), Gutmann and Hyvarinen
% Ein Gutes Modell, kann Daten und Rauschen mittels logistischer Regression unterscheiden
% Benötigt samples und numerische Wahrscheinlichkeiten der Raschverteilung 
% Neg. Sampling benötigt nur samples
% Für die Wahrsscheinlichkeitsverteilung wird in der Word2Vec Version die Unigram Verteilung ^3/4 gewählt


\section{Gradientenverfahren} \label{seq:SGD}
   Um die Zielfunktion (\ref{eq:loss}) zu optimieren bietet sich die Wahl eines Gradientenverfahrens an, da eine differenzierbare 
   Approximation des Zugehörigkeitsgrades (siehe Gleichung \ref{eq:wij}) gegeben ist.

   In den vergangenen Jahren gab es viele Weiterentwicklungen, insbesondere bezüglich der Konvergenzgeschwindigkeit, 
   von Gradientenverfahren. Diese werden unter anderem für das trainieren neuronaler Netzwerke bei der Backpropgation genutzt. 
   In \cite{SGDGPU} werden verschiedene Implementierungen verglichen.

   Um den Rechenaufwand im Gradientenverfahren zu verringern, wird der Gradient in zwei Summanden aufgeteilt. Dabei wird folgende Beobachtung 
   genutzt:
   Für Kanten $\{i,j\}$ mit einem hohen Zugehörigkeitsgrad $(\vij \approx 1)$ ist der Term $(1-\vij) \log(1-\wij)$ aus Gleichung (\ref{eq:loss}) 
   nahe Null, deshalb ist es sinnvoll nur den Gradienten des Terms $\vij \log(\wij)$ zu betrachten. Für Kanten mit $\vij \approx 0)$ sollte 
   hingegen der Gradient des Terms $(1-\vij) \log(1-\wij)$ betrachtet werden. 
   Für das UMAP Verfahren wird deswegen folgende Implementierung vorgeschlagen:

   \begin{algorithm}
      \caption{Optimiere die Einbettung mittels modifiziertem SGD}
      \label{algorithm:optimize}
      \begin{algorithmic}[1]
      \Function{OptimiereEinbettung}{$Y, V, W, \text{n\_epochs}$}
         \State $\alpha \gets 1.0$
         \For{$e \gets 1,\dots,\text{n\_epochs}$}
            \ForAll{$\vij $} % TODO: !! stimmt das?
               \If{$\text{Random()} \leq \vij$} \label{alg:rand}
                  \State $\mathbf{y}_i \gets \mathbf{y}_i + \alpha \cdot \nabla (\log(\wij))$ \label{alg:up1}
                  \For{$l \gets 1,\dots,\text{n\_neg\_samples}$}
                     \State $m \gets \mathcal{U}\text{nif}((0,N))$
                     \State $\mathbf{y}_i \gets \mathbf{y}_i + \alpha \gamma \cdot \nabla (\log(1-w_{im}))$ \label{alg:up2}
                  \EndFor
               \EndIf
            \EndFor
         \EndFor
         \State $\alpha \gets 1.0 - e/\text{n\_epochs}$
         \State \textbf{return} $Y$
      \EndFunction
      \end{algorithmic}
      \end{algorithm}

   Die in Zeile \ref{alg:rand} beschriebene Ziehung der Stichprobe dient dazu in Zeile \ref{alg:up1} keine zusätzliche Multiplikation 
   mit $\vij$ zu machen. Diese Idee entstammt \cite{LINE}. Dadurch wird der Gradient nicht durch den Wert von $\vij$ beeinflusst. % TODO: !! Besser begründen siehe 4.2.1 in LINE

   Das in jedem Durchlauf des modifizierten SGD mehrere \textit{negative samples} betrachtet werden geht auch \cite{Mikolov} zurück. 
   Im wesentlichen verhindert dies, das sich die Vektoren der niedrigdimensionalen Darstellung häufen. Die dabei $(0,N)$-gleichverteilt 
   gezogene Stichprobe ist eine Modifizierung von \cite{Tang}. % TODO: besser begründen.

   Der Gradient in Zeile \ref{alg:up1} ist gegeben durch: 

   \begin{equation}
      \nabla (\log(\wij)) = \frac{-2 a b \norm{\mathbf{y}_i - \mathbf{y}_j}^{2(b-1)}_2 }{(1+a(\norm{\mathbf{y}_i - \mathbf{y}_j}^2_2)^b)^{-1}} (\mathbf{y}_i - \mathbf{y}_j)
   \end{equation}

   und der Gradient in Zeile \ref{alg:up2} durch: 
 
   \begin{equation}
      \nabla (1-\log(\wij)) = \frac{2 b}{(\epsilon + \norm{\mathbf{y}_i - \mathbf{y}_j}^{2}_2)(1 + a\norm{\mathbf{y}_i - \mathbf{y}_j}^{2b}_2)} (\mathbf{y}_i - \mathbf{y}_j),
   \end{equation}

   wobei der $\epsilon$-Parameter eine Division mit Null vermeidet.




   % Das UMAP Verfahren nutzt für die Optimierung der Zielfunktion die \textit{negative sampling} Methode aus dem Bereich 
   % der Worteinbettungen \cite{Mikolov}. Wir werden diese Methode vorstellen und begründen warum sich die Argumentation aus dem 
   % Bereich der Worteinbettungen auf die Dimensionsreduktion übertragen lässt.
   % Dann vervollständigen Algorithmus \ref{algorithm:umap} um die Subroutine \textsc{OptimiereEinbettung}.
   
   % % Optimiert Suchrichtung des Gradienten da wichtige Beispiele betrachtet werden. 
   % \subsection*{Negative Sampling}
   %    Die erste uns bekannte Quelle des negative sampling ist Mikolov et. al \cite{Mikolov}. Das Methode wird 
   %    für die Problemstellung entwickelt um für ein Paar $(w, c)$ eines Wortes $w$ und eines dazugehörigen Kontextes $c$ 
   %    eine Vektorrepräsentation zu finden. Dafür ist uns eine Sprache $W$ und eine Kontextmenge $C$ gegeben, wobei für 
   %    $w \in W$ die Menge $C(w)$ alle zu w passenden Kontexte angibt. Eine sinnvolle Repräsentation der Worte \enquote{Numerik}, 
   %    \enquote{Garcke}, \enquote{Algebraische Topologie}, \enquote{Scholze} wäre beispielsweise 
   %    $vec(\text{Algebraische Topologie}) - vec(\text{Scholze}) + vec(\text{Numerik}) = vec(\text{Garcke})$

   % \subsection*{Optimierung des UMAP Gradienten}
   %    Damit können wir Algorithmus \ref{algorithm:umap} vervollständigen.

   %    \begin{algorithm}
   %    \caption{OptimiereEinbettung}
   %    \label{algorithm:optimize}
   %    \begin{algorithmic}[1]
   %    \Function{OptimiereEinbettung}{$Y, V, W, \text{n\_epochs}$}
   %       \State $\alpha \gets 1.0$
   %       \For{$e \gets 1,\dots,\text{n\_epochs}$}
   %          \State $x \gets \dots$ % TODO: ergänze
   %       \EndFor
   %       \State $\alpha \gets 1.0 - e/\text{n\_epochs}$
   %       \State \textbf{return} $Y$
   %    \EndFunction
   %    \end{algorithmic}
   %    \end{algorithm}

   %    In den uns bekannten Implementierungen wird der Gradient in Zeile REF?? außerhalb des Intervalls 
   %    $[-4,4]$ auf $-4$ bzw. $4$ gesetzt, um die Auswirkung von schlechten Kanten zu reduzieren \cite{Goodfellow}. % TODO: Referenz


   %    Für dünn-besetzte Datensätze $X$ könnte man beispielsweise das AdaGrad Verfahren betrachten, 
   %    dieses zeigte in der Praxis sehr gute Ergebnisse für dünn-besetzte Daten. % TODO: Quellen

%----------------------------------------------------------------------------------------

\section{Nächste-Nachbarn-Klassifikation} \label{seq:kNN}
   Zum effizienten finden der 1-Simplizes der topologischen Repräsentation unserer Daten, benötigen wir einen 
   k-nächste-Nachbarn-Algorithmus (kurz: \textit{kNN-Algorithmus}). 
   %TODO: Beschreibung was ein kNN-Alg macht

   Das Ergebnis eines kNN-Algorithmus wird meist in einem ungerichteten Graph -- dem kNN-Graph -- dargestellt, 
   wobei die Knoten den Datenpunkten entsprechen und die Kanten den Nachbarschaftsbeziehungen, 
   somit besitzt jeder Knoten Grad k.

   Bei einer naiven Implementierung beträgt die Laufzeit $\mathcal{O}(N^2D)$ (wobei N die Anzahl der Datenpunkte  % TODO: warum N^2D?
   und D die Dimension der Datenpunkte ist). Mit einer effizienten Implementierung ist in der Praxis eine 
   annähernd in N lineare Laufzeit möglich. Die Herangehensweisen lassen sich nach \cite{Tang} in drei Kategorien 
   einteilen. (1) Baum basierte Verfahren auf Partitionen des Raumes, (2) Hashfunktionen auf lokalen Teilgebieten des Raumes 
   (3) Nachbarschafts-Erkundungen. 

   Wir möchten nun zwei Verfahren vorstellen. 

   %-----------------------------------

   \subsection*{NN-Descent}
      Der NN-Descent Algorithmus \cite{k-NNG} beruht auf dem Prinzip der Nachbarschafts-Erkundungen. Dabei wird ein 
      initialer kNN-Graph iterativ verbessert, unter der Annahme, dass die Nachbarschaftsbeziehung 
      transitiv ist, für zwei vorhandene Nachbarschaftspaare $(x, y), (y,z)$ also mit hoher Wahrscheinlichkeit auch 
      ein Nachbarschaftspaar $(x,z)$ im kNN-Graph existiert. 
      Der initiale Graph im NN-Descent Verfahren wird dabei zufällig gewählt. Dies kann dazu führen, dass nur 
      lokal optimale k-NN-Graphen gefunden werden. Dies könnte laut \cite{EFANNA} dadurch verbessert werden, indem 
      für die Initialisierung \enquote{random projection trees}, wie in \cite{Tang}, verwendet werden. 

      Ein Vorteil des NN-Descent Verfahren ist, dass kein globaler Index der verwaltet werden muss. Somit ist eine 
      Anwendung auf großen Datensätzen möglich welche nicht komplett in den Arbeitsspeicher (RAM) des verwendeten Rechners 
      geladen werden können. 

      Nachteil des NN-Descent Algorithmus ist die Speicherplatzkomplexität, diese ist durch $\mathcal{O}(N^2)$ beschränkt. 
      Im wesentlichen ist dies dadurch begründet, dass paarweise die Ähnlichkeit, welche im Falle von UMAP durch die Metrik 
      des Umgebungsraums gegeben ist, gespeichert wird. Aufgrund dessen, dass nur lokale Optima garantiert sind, ist das Ergebnis 
      des NN-Descent Verfahren approximativ. In \cite{UMAP} wird jedoch argumentiert, dass dies wegen des Informationsverlust 
      bei Dimensionsreduktionen kaum Auswirkungen auf die resultierende Einbettung hat. 

   %-----------------------------------

   \subsection*{FAISS}
      Das FAISS Bibliothek \cite{FAISS} nutzt die Architektur einer GPU aus. Dabei baut FAISS eine effiziente Datenstruktur, 
      welche für die Vektoren die nächsten Nachbarn speichert. Somit ist eine sehr schnelle Implementierung 
      für das aufstellen des k-NN-Graphen möglich. 

      Der RAM der meisten GPUs ist stark begrenzt. Um dennoch mit großen Datensätzen zu arbeiten werden komprimierte 
      Darstellungen der Vektoren genutzt. % TODO: Referenzen einfügen, aus FAISS
      Für FAISS werden \enquote{product quantization codes} genutzt. % TODO: referenz und kurze Beschreibung 

      Vorteile der FAISS Datenstruktur sind die effiziente Implementierung auf GPUs und das sowohl exakte Ergebnisse 
      sowie Approximationen für die nächsten Nachbarn angegeben werden können. Die Rückgabe approximativer 
      Ergebnisse erhält Laufzeit sowie Speicherplatz Vorteile. 

      Nachteil des FAISS Verfahren ist, dass zurzeit nur die euklidische Distanz unterstützt wird. 

%----------------------------------------------------------------------------------------

\section{Hyperparameter} \label{seq:hyper}

   \begin{itemize}
      \item \code{n\_neighbors}:
            beschreibender Text
        % The size of local neighborhood (in terms of number of neighboring
        % sample points) used for manifold approximation. Larger values
        % result in more global views of the manifold, while smaller
        % values result in more local data being preserved. In general
        % values should be in the range 2 to 100.
      \item $\code{metric}$
         Text
      \item $\code{n\_epochs}$
         Text
      \item \code{set\_op\_mix\_ratio}
        % Interpolate between (fuzzy) union and intersection as the set operation
        % used to combine local fuzzy simplicial sets to obtain a global fuzzy
        % simplicial sets. Both fuzzy set operations use the product t-norm.
        % The value of this parameter should be between 0.0 and 1.0; a value of
        % 1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy
        % intersection.
      \item $\code{local\_connectivity}$ 
         % The local connectivity required -- i.e. the number of nearest
         % neighbors that should be assumed to be connected at a local level.
         % The higher this value the more connected the manifold becomes
         % locally. In practice this should be not more than the local intrinsic
         % dimension of the manifold.
      \item \code{repulsion\_strength}
         % Weighting applied to negative samples in low dimensional embedding
         % optimization. Values higher than one will result in greater weight
         % being given to negative samples.
      \item \code{a, b}:
            Erwähne wie a und b standardmäßig gewählt werden, zeige Plot von a, b
            Erwähne \code{min\_dist} und \code{spread}
      \item \code{negative\_sample\_weight}
   \end{itemize}

%----------------------------------------------------------------------------------------      
