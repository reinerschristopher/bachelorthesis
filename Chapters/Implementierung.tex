%!TEX root = ../main.tex

\chapter{Implementierung} \label{Implementierung}

%----------------------------------------------------------------------------------------

% N O T I Z E N
   % Cite Numba 24

   % TODO: Bem: Fehler in Kapitel 3 von UMAP wij sollten gewichte der Matrix B sein.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

% \section{Einleitung}
   In diesem Kapitel möchten wir die Implementierung des UMAP Verfahren beschreiben. 
   Ideal-typischerweise würden wir dafür die metrischen Räume konstruieren und diese mittels 
   des modifizierten Singulären Mengen Funktors mit der in Gleichung (\ref{eq:toprep}) beschriebenen 
   unscharfen topologischen Repräsentation darstellen. Praktikabel ist dies nicht für große Datensätze 
   umsetzbar, da wir für die Berechnung der unscharfen simplizialen Mengen alle 
   $2^N$ Teilmengen unseres $N$-elementigen Datensatzes betrachten müssten. 

   In aktuellen Implementierungen werden hingegen nur alle zweielementigen Teilmengen betrachtet. 
   Wir werden in Kapitel \ref{Zusammenfassung} Ansätze erwähnen um Simplizes höherer 
   Ordnungen effizient zu finden. 

   Zunächst werden wir in Abschnitt \ref{seq:numerische_formulierung} die Notation anpassen und 
   die Formulierung der Optimierung konkretisieren.
   Wir werden dann mittels Profiling, in Abschnitt \ref{sec:profile}, die rechenintensiven Subroutinen 
   der von uns verwendeten Implementierung \cite{cpu} identifizieren. In den darauffolgenden Abschnitten 
   werden wir die aufwendigen Routinen genauer betrachten, in dem wir in Abschnitt \ref{seq:SGD} 
   das verwendete Verfahren zur Optimierung der Einbettung beschreiben und in Abschnitt \ref{seq:kNN} 
   zwei Verfahren zu Berechnung der $1$-Simplizes diskutieren.
   Abschnitt \ref{seq:hyper} gibt eine Beschreibung der Hyperparameter an. 

%----------------------------------------------------------------------------------------

\section{Numerische Formulierung des UMAP Verfahrens} \label{seq:numerische_formulierung}
   Um weitere Überlegungen zu vereinfachen und die praktische Implementierung zu beschreiben, werden wir nun die Notation anpassen.

   Dabei ist zu bemerken, dass wir uns von nun an auf die Konstruktion des $1$-Skelett der unscharfen simplizialen Mengen beschränken. 
   Die Vereinfachung, dass wir keine Simplizes höherer Ordnungen betrachten werden wir in Abschnitt \ref{sec:ausblick} betrachten 
   und Ansätze erwähnen, um Simplizes höherer Ordnungen zu nutzen. 

   Wir werden von nun an die $0$-Simplizes einer (unscharfen) Menge als Knoten, die $1$-Simplizes als Kanten, das $1$-Skelett als Graph 
   und den Zugehörigkeitsgrad einer Kante als Gewicht der Kante bezeichnen. 

   Sei $\Xrep$ die in Gleichung (\ref{eq:toprep}) definierte topologische Repräsentation der Daten aus $X$ und sei $k \in \N$ gegeben. 

   Das $1$-Skelett von $\Xrep$ lässt sich durch einen Graph $\Xg$ beschreiben. Dafür benötigen wir einen Zwischenschritt, in welchem wir 
   $N$ Graphen $\Xgi$ konstruieren, diese beschreiben das Bild $\fSing((X,d_i))$. Dabei sind die Gewichte von $\Xgi$ durch,

   \begin{align}
      \wXi(x,y) \coloneqq \begin{cases}
                           \exp \left(\frac{-\max(0, d(x,y)-\rho_i)}{\sigma_i}\right) \quad &\text{,falls } x=\xxi, y=\mathbf{x}_{i_j}\\
                           0 \quad &\text{,sonst}
                         \end{cases}
   \end{align}

   mit $1 \leq j \leq k$, gegeben. $\mathbf{x}_{i_j}$ ist wieder der $j$-te Nachbar von $\xxi$, $\rho_i$ ist die Distanz von $\xxi$ 
   zum ersten Nachbarn. In Kapitel \ref{UMAP} wurde $\sigma_i$ als Distanz zum $k$-ten Nachbarn gewählt. 
   In der Praxis werden wir diese Wahl leicht modifizieren, indem wir $\sigma_i$ so wählen, dass folgende Gleichung erfüllt ist: % TODO: warum?

   \begin{align}
      \sum_{j=1}^k \exp \left(\frac{-\max(0, d(\xxi,\mathbf{x}_{i_j})-\rho_i)}{\sigma_i}\right) = \log_2(k)
   \end{align}

   Der gewichtete Graph $\Xgi$ besitzt also die Knotenmenge $V(\Xgi)=X$ und die Kantenmenge $E(\Xgi)=(X \times X)$ mit Kantengewichten $\wXi$. 
   % Mit $W_i$ bezeichnen wir die zu $\Xgi$ gehörende gewichtete Adjazenzmatrix. 

   Die Vereinigung der $\Xgi$ zu $\Xg$ lässt sich als Anwendung der t-Conorm auf die Gewichte beschreiben. Somit erhalten wir, für $x,y \in X$ eine neue 
   Gewichtsfunktion $\wX$, mit 

   \begin{align}
      \wX(x,y) \coloneqq w_{\mathcal{X}_1}(x,y) \perp w_{\mathcal{X}_2}(x,y) \perp \dots \perp w_{\mathcal{X}_N}(x,y).
   \end{align}

   Um in Abschnitt \ref{sec:einb} die Wahl der Kreuzentropie zu begründen, haben wir 
   argumentiert, dass man die Zugehörigkeitsfunktionen der Simplizes als Wahrscheinlichkeitsverteilung auf den Simplizes betrachten kann. 
   Diese Argumentation kann man auch anwenden um die Wahl der t-Conorm, aus Beispiel \ref{ex:tNorm}, gegeben durch 
   $\bot(a,b) \coloneqq a + b - ab$, zu begründen. Diese entspricht nämlich genau der Wahrscheinlichkeit der Vereinigung unabhängiger 
   Ereignisse. 
   % Nun können wir die Gewichte von $\Xg$ explizit angeben, 

   % \begin{align}
   %    \wX(x,y) \coloneqq \text{FEHLT} %\sum_{i=1}^N \wXi(x,y) - \sum_{i=2}^N w_1(x,y) \prod_{j=2}^N w_j(x,y). % TODO: ergänzen
   % \end{align}

   Wir werden nun $\Yg$ aus Gleichung (\ref{eq:lowrep}) konstruieren. Dafür setzen wir 

   \begin{align}
      \Psi(x,y) \coloneqq 
      \begin{cases}
         1 \quad & \text{, falls } \norm{x-y}_2 \leq \text{min-dist} \\
         \exp(-(\norm{x-y}_2 - \text{min-dist})) \quad & \text{, sonst.}
      \end{cases}
      \label{eq:Psi}
   \end{align}

   Die Wahl des Hyperparameter min-dist wird später diskutiert. Dieser dient dazu, eine ähnliche Transformation wie im 
   Gleichung (\ref{eq:di}) durchzuführen. Um später die Kreuzentropie zwischen $\Xg$ und $\Yg$ zu minimieren muss die 
   Gewichtsfunktion von $\Yg$ differenzierbar sein. Offensichtlich ist $\Psi$ bei min-dist nicht differenzierbar. 
   Somit betrachten wir eine stetige Approximation von $\Psi$,

   \begin{align} % TODO: Plot
      \wY(x,y) \coloneqq (1 + a\norm{x-y}_2^{2b})^{-1}, 
      \label{eq:wY}
   \end{align}

   wobei $a$ und $b$ Hyperparameter sind. Die Wahl von $a$ und $b$ kann beispielsweise mittels der Methode der kleinsten Quadrate 
   bezüglich $\Psi$ optimiert werden. 

   Die Kreuzentropie lässt sich auf gewichtete Graphen übertragen, wenn wir die Gewichte als Zugehörigkeitsgrade interpretieren,

   \begin{alignat}{2}
      C_\text{cross}(\Xg, \Yg) &\coloneqq \sum_{e \in X \times X} \wX(e) \log\left(\frac{\wX(e)}{\wY(e)}\right) + (1-\wX(e)) \log\left(\frac{1-\wX(e)}{1-\wY(e)}\right) \\
                               &= \begin{aligned}[t]
                                    & \sum_{e \in X \times X} \wX(e) \log(\wX(e)) + (1-\wX(e)) \log(1-\wX(e)) \\
                                    &- \sum_{e \in X \times X} \wX(e) \log(\wY(e)) + (1-\wX(e)) \log(1-\wY(e))
                                  \end{aligned} \\
                               &= C_\mathcal{X} - \sum_{e \in X \times X} \wX(e) \log(\wY(e)) + (1-\wX(e)) \log(1-\wY(e)) \label{eq:loss}
   \end{alignat}

   Dabei ist $C_\mathcal{X}$ von $\Yrep$ unabhängig und somit bei der Minimierung der Kreuzentropie nicht von Relevanz. 
   Eine Modifizierung des stochastischen Gradienten Verfahrens zur Minimierung von Gleichung (\ref{eq:loss}) werden wir in Abschnitt \ref{seq:SGD} betrachten.

   Die Überlegungen können wir nun im Pseudo-Code zusammenfassen. 

   \begin{algorithm}
   \caption{UMAP Algorithmus}
   \label{algorithm:umap}
   \begin{algorithmic}[1]
   \Function{UMAP}{$X, N, D, d, \code{min-dist}, \code{n-epochs}$}
      \For{$\xxi \in X$}
         \State $knn(\xxi) \gets k\text{-nächste-Nachbarn}(\xxi)$ \label{alg:kNN}
         \State $graph(\xxi) \gets (\Xgi, \wXi)$ \label{alg:graph}
      \EndFor
      \State $\Xg \gets \text{gewichtete Adjazenzmatrix}(\tConorm graph(\xxi))$ \label{alg:adj}
      \State $D \gets \text{Grad-Matrix des Graphen } \Xg$ \label{alg:grad}
      \State $L \gets D^{1/2} (D-\Xg) D^{1/2}$ \Comment{Symmetrische normalisierte Laplace-Matrix}\label{alg:lapl}
      \State $evec \gets \text{sortierte Eigenvektoren von } L$ \label{alg:evec}
      \State $Y \gets evec[1,\dots,d\text{+}1]$ \label{alg:spec}
      \State $Y \gets \textsc{OptimiereEinbettung}(Y, \code{min-dist}, \code{n-epochs})$ \Comment{siehe Alg. \ref{algorithm:optimize}} \label{alg:opt}
      \State \textbf{return} $Y$
   \EndFunction
   \end{algorithmic}
   \end{algorithm}

   In Abschnitt \ref{seq:kNN} werden zwei effiziente Verfahren für die k-nächste-Nachbarn Suche (Zeile \ref{alg:kNN}) angeben. 

   Die in Zeile \ref{alg:graph} beschriebene Form die Graphen der $\xxi$ zu konstruieren scheint auf den ersten Blick sehr viel Speicher zu benötigen, 
   nämlich $\mathcal{O}(N^3)$. Dabei wird anstatt $N$ nur eine Adjazenzmatrix $A$ benötigt, in $A$ mit $a_{ij} = \wXi(\xxi,\xxj)$. Wir haben zuvor 
   die Notation mit $N$ Adjazenzmatrizen gewählt, um die Verbindung mit der Konstruktion des UMAP Verfahrens aus Kapitel \ref{UMAP} zu verdeutlichen. 
   Das betrachten von $A$ ermöglicht uns eine effiziente Implementierung der t-Conorm in Zeile \ref{alg:adj}, dabei gilt $\Xg = A + A^\top - A \circ A^\top$, 
   wobei $\circ$ das elementweise Produkt ist.

   Die Zeilen \ref{alg:grad} - \ref{alg:spec} beschreiben eine Alternative Initialisierung von $Y$ mit der spektralen Einbettung der symmetrischen Laplace-Matrix. 
   Die Grad-Matrix $D$ in Zeile \ref{alg:grad} ist eine Diagonalmatrix, wobei  
   $d_{ii} \coloneqq  \sum_{1 \leq j \leq N} \wX(\xxi, \xxj), (1 \leq i \leq N)$. 
   Für den Spezialfall, einer ungewichteten Adjazenzmatrix, beschreibt $d_{ii}$ also den Grad des Knoten $i$. Die in Zeile \ref{alg:lapl} 
   beschriebene Matrix $L$ nennt man symmetrische normalisierte Laplace-Matrix. Die Eigenwerte einer Adjazenzmatrix nennt man das \textit{Spektrum des Graphen}. 
   Dabei wählt man den Spektrale Einbettung der Laplace-Matrix, da diese eine diskrete Form des Laplace-Beltrami Operators ist \cite{Chung}. 
   Der Laplace-Beltrami Operator ist eine Erweiterung des Laplace Operators auf Riemannsche Mannigfaltigkeiten. % TODO: Überarbeiten
   Dabei ist zu beachten, dass eine Lösung des Eigenwertproblems nur von der Größe der Einbettungsdimension abhängig ist. Da wird stets $d \ll D$ 
   betrachten ist eine effiziente Implementierung mittels sukzessiver Eigenwertsuche möglich. 

   Eine Beschreibung der Optimierung in Zeile \ref{alg:opt} liefern wir in Abschnit \ref{seq:SGD}.

%----------------------------------------------------------------------------------------

\section{Profiling} \label{sec:profile}
   In Kapitel \ref{Experimente} werden wir genauer auf die tatsächliche Laufzeit des UMAP Algorithmus eingehen. 
   An dieser Stelle möchten wir die rechenintensiven Subroutinen des Verfahrens ausmachen. Dafür haben wir den 
   Python cProfiler verwendet, dieser misst die Laufzeit der aufgerufenen Funktionen. 
   Um für verschiedene Umgebungsdimensionen vergleichbare Ergebnisse zu erhalten, haben wir $N = \num{10000}$ 
   Datenpunkte in $10$ unterschiedlichen $D = [100, 500, 1000, 5000, 10000, 50000]$-dimensionalen Gauß-verteilten 
   Datenwolken gewählt. Diese Daten wurden dann in den zweidimensionalen Raum eingebettet.
   
   Dabei ist uns aufgefallen, dass besonders der k-nächste-Nachbarn-Algorithmus und die Optimierung mittels 
   stochastischem Gradienten Verfahren einen großen Teil der Laufzeit des Verfahrens beanspruchen. In Tabelle 
   \ref{table:profiling} sind die Ergebnisse der Profilierung zusammengefasst. 
   Insbesondere scheint die k-nächste-Nachbarn Suche die Laufzeit des UMAP Verfahren für 
   hochdimensionale Daten stark zu beeinflussen. 
   Wir werden beide rechenintensiven Subroutinen im folgenden betrachten und geeignete Verbesserungen diskutieren.

   \begin{table}
   \centering
   \begin{tabular}{l|ll}
   D     & Laufzeit NN-Descent & Laufzeit der Optimierung \\ \hline
   100   & 9\%                 & 75,3\%                   \\
   500   & 12\%                & 73,8\%                   \\
   1000  & 14\%                & 72,9\%                   \\
   5000  & 30,4\%              & 58\%                     \\
   10000 & 44\%                & 45,1\%                   \\
   50000 & 78,8\%              & 14,8\%                  
   \end{tabular}
   \caption{$D$ beschreibt die Größe der Umgebungsdimension. Abhängig von D haben wir die Laufzeit des UMAP Verfahrens 
            profiliert. Die zweite und dritte Spalte beziehen sich auf die relativen Laufzeiten des kNN Verfahrens und 
            der Optimierung der Einbettung.}
   \label{table:profiling}
   \end{table} 

%----------------------------------------------------------------------------------------

% N O T I Z E N
% sub-sampling bei Wortrepräsentationen sorgt für verbesserte Laufzeiten (2x - 10x)
% und Genauigkeit weniger häufig repräsentierter Wörter nimmt zu

% Negative sampling
% Noise Contrastive Estimation (NCE), Gutmann and Hyvarinen
% Ein Gutes Modell, kann Daten und Rauschen mittels logistischer Regression unterscheiden
% Benötigt samples und numerische Wahrscheinlichkeiten der Raschverteilung 
% Neg. Sampling benötigt nur samples
% Für die Wahrsscheinlichkeitsverteilung wird in der Word2Vec Version die Unigram Verteilung ^3/4 gewählt


\section{Gradientenverfahren} \label{seq:SGD}
   Um die Zielfunktion aus Gleichung (\ref{eq:loss}) zu minimieren bietet sich die Wahl eines Gradientenverfahrens an, da eine differenzierbare 
   Approximation des Zugehörigkeitsgrades (siehe Gleichung \ref{eq:wY}) gegeben ist.

   In den vergangenen Jahren gab es viele Weiterentwicklungen, insbesondere bezüglich der Konvergenzgeschwindigkeit, 
   von Gradientenverfahren. Diese werden unter anderem für das trainieren neuronaler Netzwerke bei der Backpropgation genutzt. 
   In \cite{SGDGPU} werden verschiedene Implementierungen verglichen.

   Für das UMAP Verfahren wird eine modifizierte Version des stochastischen Gradientenverfahrens angewendet. Diese möchten wir kurz beschreiben. 
   Um den Rechenaufwand im Gradientenverfahren zu verringern, wird der Gradient in zwei Summanden aufgeteilt. Dabei wird folgende Beobachtung 
   genutzt:
   Für Kanten $\{i,j\}$ mit einem hohen Zugehörigkeitsgrad $(\wX \approx 1)$ ist der Term $(1-\wX) \log(1-\wY)$ aus Gleichung (\ref{eq:loss}) 
   nahe Null, deshalb ist es sinnvoll nur den Gradienten des Terms $\wX \log(\wY)$ zu betrachten. Für Kanten mit $\wX \approx 0)$ sollte 
   hingegen der Gradient des Terms $(1-\wX) \log(1-\wY)$ betrachtet werden. 
   Die Idee dieser Optimierung stammt aus \cite{Mikolov} und dort zum finden eines Kontext für ein gegebenes Wort verwendet. Es wird eine 
   Beschleunigung der Optimierung um den Faktor $2x - 10x$ erzielt. 
   Für das UMAP Verfahren wird deswegen folgende Implementierung vorgeschlagen:

   \begin{algorithm}
      \caption{Optimiere die Einbettung mittels modifiziertem SGD}
      \label{algorithm:optimize}
      \begin{algorithmic}[1]
      \Function{OptimiereEinbettung}{$Y, V, W, \text{n-epochs}$}
         \State $\alpha \gets 1.0$
         \For{$e \gets 1,\dots,\text{n-epochs}$}
            \ForAll{$\{\yyi, \yyj\}$} % TODO: !! stimmt das?
               \If{$\text{Random()} \leq \wX(\yyi, \yyj)$} \label{alg:rand}
                  \State $\yyi \gets \yyi + \alpha \cdot \nabla (\log(\wY))(\yyi, \yyj)$ \label{alg:up1}
                  \For{$l \gets 1,\dots,\text{n-neg-samples}$}
                     \State $m \gets \mathcal{U}\text{nif}((0,N))$
                     \State $\yyi \gets \yyi + \alpha \cdot \gamma \cdot \nabla (\log(1-\wY))(\yyi, \mathbf{y}_m)$ \label{alg:up2}
                  \EndFor
               \EndIf
            \EndFor
         \EndFor
         \State $\alpha \gets 1.0 - e/\text{n-epochs}$
         \State \textbf{return} $Y$
      \EndFunction
      \end{algorithmic}
      \end{algorithm}

   Die in Zeile \ref{alg:rand} beschriebene Ziehung der Stichprobe dient dazu in Zeile \ref{alg:up1} keine zusätzliche Multiplikation 
   mit $\wX$ zu machen. Diese Idee entstammt \cite{LINE}. Dadurch wird der Gradient nicht durch den Wert von $\wX$ beeinflusst. % TODO: !! Besser begründen siehe 4.2.1 in LINE

   Das in jedem Durchlauf des modifizierten SGD mehrere \textit{negative samples} betrachtet werden geht auch \cite{Mikolov} zurück. 
   Im wesentlichen verhindert dies, das sich die Vektoren der niedrigdimensionalen Darstellung häufen. Die dabei $(0,N)$-gleichverteilt 
   gezogene Stichprobe ist eine Modifizierung von \cite{Tang}. Die Wahl des Hyperparameter n-neg-samples soll laut \cite{Mikolov} zwischen zwei und $20$ liegen.  % TODO: besser begründen.

   Der Gradient in Zeile \ref{alg:up1} ist gegeben durch: 

   \begin{equation}
      \nabla (\log(\wY))(\yyi, \yyj) = \frac{-2 a b \norm{\mathbf{y}_i - \mathbf{y}_j}^{2(b-1)}_2 }{(1+\norm{\mathbf{y}_i - \mathbf{y}_j}^{2}_2)} (\mathbf{y}_i - \mathbf{y}_j)
   \end{equation}

   und der Gradient in Zeile \ref{alg:up2} durch: 
 
   \begin{equation}
      \nabla (1-\log(\wY))(\yyi, \yyj) = \frac{2 b}{(\epsilon + \norm{\mathbf{y}_i - \mathbf{y}_j}^{2}_2)(1 + a\norm{\mathbf{y}_i - \mathbf{y}_j}^{2b}_2)} (\mathbf{y}_i - \mathbf{y}_j),
   \end{equation}

   wobei der $\epsilon$-Parameter eine Division mit Null vermeidet.

   Dies vervollständigt die Angabe des UMAP Verfahrens in der Praxis. Die genannten Hyperparameter werden wir in Abschnitt \ref{seq:hyper} betrachten.

%    Für dünn-besetzte Datensätze $X$ könnte man beispielsweise das AdaGrad Verfahren betrachten, 
%    dieses zeigte in der Praxis sehr gute Ergebnisse für dünn-besetzte Daten. % TODO: Quellen

%----------------------------------------------------------------------------------------

\section{Nächste-Nachbarn-Klassifikation} \label{seq:kNN}
   Zum effizienten finden der 1-Simplizes der topologischen Repräsentation unserer Daten, benötigen wir einen 
   k-nächste-Nachbarn-Algorithmus (kurz: \textit{kNN-Algorithmus}). 
   %TODO: Beschreibung was ein kNN-Alg macht

   Das Ergebnis eines kNN-Algorithmus wird meist in einem ungerichteten Graph -- dem kNN-Graph -- dargestellt, 
   wobei die Knoten den Datenpunkten entsprechen und die Kanten den Nachbarschaftsbeziehungen, 
   somit besitzt jeder Knoten Grad k.

   Bei einer naiven Implementierung beträgt die Laufzeit $\mathcal{O}(N^2D)$ (wobei N die Anzahl der Datenpunkte  % TODO: warum N^2D?
   und D die Dimension der Datenpunkte ist). Mit einer effizienten Implementierung ist in der Praxis eine 
   annähernd in N lineare Laufzeit möglich. Die Herangehensweisen lassen sich nach \cite{Tang} in drei Kategorien 
   einteilen. (1) Baum basierte Verfahren auf Partitionen des Raumes, (2) Hashfunktionen auf lokalen Teilgebieten des Raumes 
   (3) Nachbarschafts-Erkundungen. 

   Wir möchten nun zwei Verfahren vorstellen. 

   %-----------------------------------

   \subsection*{NN-Descent}
      Der NN-Descent Algorithmus \cite{k-NNG} beruht auf dem Prinzip der Nachbarschafts-Erkundungen. Dabei wird ein 
      initialer kNN-Graph iterativ verbessert, unter der Annahme, dass die Nachbarschaftsbeziehung 
      transitiv ist, für zwei vorhandene Nachbarschaftspaare $(x, y), (y,z)$ also mit hoher Wahrscheinlichkeit auch 
      ein Nachbarschaftspaar $(x,z)$ im kNN-Graph existiert. 
      Der initiale Graph im NN-Descent Verfahren wird dabei zufällig gewählt. Dies kann dazu führen, dass nur 
      lokal optimale k-NN-Graphen gefunden werden. Dies könnte laut \cite{EFANNA} dadurch verbessert werden, indem 
      für die Initialisierung \enquote{random projection trees}, wie in \cite{Tang}, verwendet werden. 

      Ein Vorteil des NN-Descent Verfahren ist, dass kein globaler Index der verwaltet werden muss. Somit ist eine 
      Anwendung auf großen Datensätzen möglich welche nicht komplett in den Arbeitsspeicher (RAM) des verwendeten Rechners 
      geladen werden können. 

      Nachteil des NN-Descent Algorithmus ist die Speicherplatzkomplexität, diese ist durch $\mathcal{O}(N^2)$ beschränkt. 
      Im wesentlichen ist dies dadurch begründet, dass paarweise die Ähnlichkeit, welche im Falle von UMAP durch die Metrik 
      des Umgebungsraums gegeben ist, gespeichert wird. Aufgrund dessen, dass nur lokale Optima garantiert sind, ist das Ergebnis 
      des NN-Descent Verfahren approximativ. In \cite{UMAP} wird jedoch argumentiert, dass dies wegen des Informationsverlust 
      bei Dimensionsreduktionen kaum Auswirkungen auf die resultierende Einbettung hat. 

   %-----------------------------------

   \subsection*{FAISS}
      Die FAISS Bibliothek \cite{FAISS} nutzt die Architektur einer GPU aus. Dabei baut FAISS eine effiziente Datenstruktur, 
      welche für die Vektoren die nächsten Nachbarn speichert. Somit ist eine sehr schnelle Implementierung 
      für das aufstellen des k-NN-Graphen möglich. 

      Der RAM der meisten GPUs ist stark begrenzt. Um dennoch mit großen Datensätzen zu arbeiten werden komprimierte 
      Darstellungen der Vektoren genutzt. % TODO: Referenzen einfügen, aus FAISS
      Für FAISS werden \textit{product quantization codes} genutzt. 
      Die Idee dieser Repräsentation ist, dass der hochdimensionale Raum in das Kartesische Produkt niedrigdimensionaler 
      Teilräume zerlegt wird. Ein Vektor im Suchraum wir dann durch kurze Sequenzen aus Indizes der Teilräume beschrieben. 
      Die euklidische Distanz zweier Datenpunkte kann anhand der Sequenzen errechnet werden, für eine genauere Beschreibung siehe \cite{FAISS2}. 
      Somit kann eine Datenstruktur verwaltet werden, welche eine schnelle Berechnung der euklidischen Distanzen ermöglicht.  

      Vorteile der FAISS Datenstruktur sind die effiziente Implementierung auf GPUs und das sowohl exakte Ergebnisse 
      sowie Approximationen für die nächsten Nachbarn angegeben werden können. Die Rückgabe approximativer 
      Ergebnisse erhält Laufzeit sowie Speicherplatz Vorteile. 

      Nachteil des FAISS Verfahren ist, dass zurzeit nur die euklidische Distanz unterstützt wird. 

%----------------------------------------------------------------------------------------

% TODO: Complete this.
\section{Hyperparameter} \label{seq:hyper}

   \begin{itemize}
      \item \code{n\_neighbors}:
            Die Größe der lokalen Nachbarschaft. Größere Werte erzielen eine globalere Ansicht der Daten. 
            Eine kleine Wahl des Parameters stellt hingegen lokale Distanzen besser dar. 
            Zusätzlich ist die Wahl des Parameters abhängig von der Anzahl der Daten. Dabei kann 
            man festhalten, je größer $N$, desto größer sollte auch \code{n\_neighbors} gewählt sein. 
      \item $\code{n-epochs}$
         Die Anzahl der Iterationen im SGD. Je größer die Wahl des Parameters, desto besser sollte 
         die Minimierung sein. Allerdings wird hier kein globales Minimum garantiert. Jedoch liefert 
         die Initiale Einbettung beruhend auf der spektralen Einbettung einen guten Startwert für die Optimierung. 
         Die Wahl der Schrittgröße im Gradientenverfahren ist antiproportional zu \code{n-epochs} gewählt und wird in 
         jeder Iteration verringert. Dies hat sich in der Praxis als sinnvolle Wahl der Schrittgröße erwiesen um nicht 
         zu schnell in lokalen Minima festzustecken. 
      \item \code{set\_op\_mix\_ratio}
        Dieser Parameter gibt an, in welchem Verhältnis eine Vereinigung mittels t-Conorm, oder ein Schnitt mittels Produkt t-Norm 
        durchgeführt werden soll. In den theoretischen Überlegungen haben wir uns stets auf die Vereinigung beschränkt. 
        Anschaulich gesehen setzen sich bei der Vereinigung, welche sich für den Fall der $1$-Skelette als Symmetrisierung beschreiben lässt 
        die größeren Gewichte durch. Somit werden Punkte in eine Zusammenhangkomponente \textit{gezwungen}. 
        Wenn man im Vergleich dazu den Schnitt, in unserem Fall die Produkt t-Norm, betrachtet werden die kleineren Gewichte bevorzugt. 
        Somit erhält man mehr Zusammenhangkomponenten. Dabei beschreibt $\code{set\_op\_mix\_ratio}=1$ die vorherigen Überlegungen nur die 
        Vereinigung zu betrachten, $\code{set\_op\_mix\_ratio}=0$ betrachtet nur den Schnitt, $\code{set\_op\_mix\_ratio}=a$ mit $a \in (0,1)$ 
        beschreiben das Verhältnis zwischen Vereinigung und Schnitt. Eine kleine Wahl kann beispielsweise dann sinnvoll sein, wenn man 
        Ausreißer in den Daten vermutet. 
      \item $\code{min-dist}$ 
         Kleine Werte des Parameters erzeugen dichtere Strukturen. Größere Distanzen sorgen dafür, das die Distanz zwischen den Punkten in der 
         Einbettung vergrößert wird. Der Startwert $0,1$ liefert meist gute Ergebnisse. Wenn viele Datenpunkte eingebettet werden sollte \code{min-dist} 
         entsprechend größer gewählt werden um ein \textit{overplotting} zu vermeiden.  
   \end{itemize}

   Wir haben die Beziehung zwischen \code{min-dist} und \code{n\_neighbors} in Abbildung \ref{fig:mindist}, anhand des MNIST Datensatzes dargestellt. 
   Dabei ist deutlich zu sehen, das die beiden Parameter Angaben über die Größe der lokalen Eigenschaften angeben. 

   \begin{figure} % Lokale struktur der Punkte in den Verfahren, cluster sehr ähnlich. cmap=nhair_color
      %\centering
      \includegraphics[width=400px, height=450px]{Figures/mindist}
      %\decoRule
      \caption[mindist]{Darstellung der Parameter \code{min-dist} und \code{n\_neighbors}. Wobei von oben nach unten: \code{min-dist} $= [0.0125,0.05,0.2,0.8]$
                        und von links nach rechts: \code{n\_neighbors} $= [5, 20, 80, 320]$}
      \label{fig:mindist}
   \end{figure}

%----------------------------------------------------------------------------------------      
