%!TEX root = ../main.tex

\chapter{Implementierung} \label{Implementierung}

%----------------------------------------------------------------------------------------

%Define math commands used in this chapter
   \newcommand{\indexij}[1][v]{{#1}_{ij}}  % Symbol for inp_{ij}
   \newcommand{\vij}{v_{ij}}  % Symbol for b_{ij}
   \newcommand{\wij}{w_{ij}}  % Symbol for w_{ij}
   \newcommand{\deriv}[2]{\frac{\partial {#1}}{\partial {#2}}}
   \newcommand{\sumtup}[2]{\sum_{{#1},{#2} = 1}^N}
   \newcommand{\sumtups}[1]{\sum_{{#1} = 1}^N}
   \newcommand{\Xg}{\mathcal{X}_G}  % Symbol for Graph of \Xrep
   \newcommand{\Xgi}{\mathcal{X}_{{G}_i}}  % Symbol for Graph of \Xrep
   \newcommand{\wX}{w_\mathcal{X}}  % Symbol for weights of \Xrep
   \newcommand{\WX}{W_\mathcal{X}}  % Symbol for Graph of \Xrep
   \newcommand{\wY}{w_\mathcal{Y}}  % Symbol for weights of \Yrep
   \newcommand{\Yg}{\mathcal{Y}_G}  % Symbol for Graph of \Xrep
   \newcommand{\xxi}{\mathbf{x}_i}
   \newcommand{\xxj}{\mathbf{x}_j}
   \newcommand{\xij}{\mathbf{x}_{i_j}}
   \newcommand{\wXi}{w_{\mathcal{X}_i}}  % Symbol for weights of \Xrep
   \newcommand{\yyi}{\mathbf{y}_i}
   \newcommand{\yyj}{\mathbf{y}_j}

%----------------------------------------------------------------------------------------

% N O T I Z E N
% Cite Numba 24

% TODO: Bem: Fehler in Kapitel 3 von UMAP wij sollten gewichte der Matrix B sein.

% \begin{align}
%    \sum_{i=1}^n \exp\left(\frac{-(\text{knn-dists}_i - \rho)}{\sigma}\right) = \log_2(n)
% \end{align}

% TODO: Daniel 
% Fallen in 4.9 alle Terme in der Summe weg für kl \neq mi ? Dann würde ich verstehen warum man auf 4.12 kommt. 
% Das würde ich dann aber noch kurz erwähnen. Und entweder in 4.11 oder 4.12 sind die Vorzeichen falsch glaub ich.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

% \section{Einleitung}
   In diesem Kapitel möchten wir die Implementierung des UMAP Verfahren beschreiben. 
   Ideal-typischerweise würden wir dafür die metrischen Räume konstruieren und diese mittels 
   des modifizierten Singulären Mengen Funktors mit der in Gleichung (\ref{eq:toprep}) beschriebenen 
   unscharfen topologischen Repräsentation darstellen. Praktikabel ist dies nicht für große Datensätze 
   umsetzbar, da wir für die Berechnung der unscharfen simplizialen Mengen alle 
   $2^N$ Teilmengen unseres $N$-elementigen Datensatzes betrachten müssten. 

   In aktuellen Implementierungen werden hingegen nur alle zweielementigen Teilmengen betrachtet. 
   Wir werden in Kapitel \ref{Zusammenfassung} Ansätze erwähnen um Simplizes höherer 
   Ordnungen effizient zu finden. 

   Zunächst werden wir in Abschnitt \ref{seq:numerische_formulierung} die Notation anpassen und 
   die Formulierung der Optimierung konkretisieren.
   Wir werden dann mittels Profiling, in Abschnitt \ref{sec:profile}, die rechenintensiven Subroutinen 
   der von uns verwendeten Implementierung \cite{cpu} identifizieren. In den darauffolgenden Abschnitten 
   werden wir die aufwendigen Routinen genauer betrachten, in dem wir in Abschnitt \ref{seq:SGD} 
   das verwendete Verfahren zur Optimierung der Einbettung beschreiben und in Abschnitt \ref{seq:kNN} 
   zwei Verfahren zu Berechnung der $1$-Simplizes diskutieren.
   Abschnitt \ref{seq:hyper} gibt eine Beschreibung der Hyperparameter an. 

%----------------------------------------------------------------------------------------

% TODO: !!!!!!! Kontroliere ob \mu(a) = \vij ist oder = den nicht symetrischen Werten. (dann ändere V und definiere \fij = sym(\vij))
\section{Numerische Formulierung des UMAP Verfahrens} \label{seq:numerische_formulierung}
   Um weitere Überlegungen zu vereinfachen und die praktische Implementierung zu beschreiben, werden wir nun die Notation anpassen.

   Dabei ist zu bemerken, dass wir uns von nun an auf die Konstruktion des $1$-Skelett der unscharfen simplizialen Mengen beschränken. 
   Die Vereinfachung, dass wir keine Simplizes höherer Ordnungen betrachten werden wir in Abschnitt \ref{sec:ausblick} betrachten 
   und Ansätze erwähnen, um Simplizes höherer Ordnungen zu nutzen. 

   Wir werden von nun an die $0$-Simplizes einer (unscharfen) Menge als Knoten, die $1$-Simplizes als Kanten, das $1$-Skelett als Graph 
   und den Zugehörigkeitsgrad einer Kante als Gewicht der Kante bezeichnen. 

   Sei $\Xrep$ die in Gleichung (\ref{eq:toprep}) definierte topologische Repräsentation der Daten aus $X$ und sei $k \in \N$ gegeben. 

   Das $1$-Skelett von $\Xrep$ lässt sich durch einen Graph $\Xg$ beschreiben. Dafür benötigen wir einen Zwischenschritt, in welchem wir 
   $N$ Graphen $\Xgi$ konstruieren, diese beschreiben das Bild $\fSing((X,d_i))$. Dabei sind die Gewichte von $\Xgi$ durch,

   \begin{align}
      \wXi(x,y) \coloneqq \begin{cases}
                           \exp \left(\frac{-\max(0, d(x,y)-\rho_i)}{\sigma_i}\right) \quad &\text{,falls } x=\xxi, y=\mathbf{x}_{i_j}\\
                           0 \quad &\text{,sonst}
                         \end{cases}
   \end{align}

   mit $1 \leq j \leq k$, gegeben. $\mathbf{x}_{i_j}$ ist wieder der $j$-te Nachbar von $\xxi$, $\rho_i$ ist die Distanz von $\xxi$ 
   zum ersten Nachbarn. In Kapitel \ref{UMAP} wurde $\sigma_i$ als Distanz zum $k$-ten Nachbarn gewählt. 
   In der Praxis werden wir diese Wahl leicht modifizieren, indem wir $\sigma_i$ so wählen, dass folgende Gleichung erfüllt ist: % TODO: warum?

   \begin{align}
      \sum_{j=1}^k \exp \left(\frac{-\max(0, d(\xxi,\mathbf{x}_{i_j})-\rho_i)}{\sigma_i}\right) = \log_2(k)
   \end{align}

   Der gewichtete Graph $\Xgi$ besitzt also die Knotenmenge $V(\Xgi)=X$ und die Kantenmenge $E(\Xgi)=(X \times X)$ mit Kantengewichten $\wXi$. 
   % Mit $W_i$ bezeichnen wir die zu $\Xgi$ gehörende gewichtete Adjazenzmatrix. 

   Die Vereinigung der $\Xgi$ zu $\Xg$ lässt sich als Anwendung der t-Conorm auf die Gewichte beschreiben. Somit erhalten wir, für $x,y \in X$ eine neue 
   Gewichtsfunktion $\wX$, mit 

   \begin{align}
      \wX(x,y) \coloneqq w_{\mathcal{X}_1}(x,y) \perp w_{\mathcal{X}_2}(x,y) \perp \dots \perp w_{\mathcal{X}_N}(x,y).
   \end{align}

   Um in Abschnitt \ref{sec:einb} die Wahl der Kreuzentropie zu begründen, haben wir 
   argumentiert, dass man die Zugehörigkeitsfunktionen der Simplizes als Wahrscheinlichkeitsverteilung auf den Simplizes betrachten kann. 
   Diese Argumentation kann man auch anwenden um die Wahl der t-Conorm, aus Beispiel \ref{ex:tNorm}, gegeben durch 
   $\bot(a,b) \coloneqq a + b - ab$, zu begründen. Diese entspricht nämlich genau der Wahrscheinlichkeit der Vereinigung unabhängiger 
   Ereignisse. 
   Nun können wir die Gewichte von $\Xg$ explizit angeben, 

   \begin{align}
      \wX(x,y) \coloneqq \text{FEHLT} %\sum_{i=1}^N \wXi(x,y) - \sum_{i=2}^N w_1(x,y) \prod_{j=2}^N w_j(x,y). % TODO: ergänzen
   \end{align}

   Wir werden nun $\Yg$ aus Gleichung (\ref{eq:lowrep}) konstruieren. Dafür setzen wir 

   \begin{align}
      \Psi(x,y) \coloneqq 
      \begin{cases}
         1 \quad & \text{, falls } \norm{x-y}_2 \leq \text{min-dist} \\
         \exp(-(\norm{x-y}_2 - \text{min-dist})) \quad & \text{, sonst.}
      \end{cases}
      \label{eq:Psi}
   \end{align}

   Die Wahl des Hyperparameter min-dist wird später diskutiert. Dieser dient dazu, eine ähnliche Transformation wie im 
   Gleichung (\ref{eq:di}) durchzuführen. Um später die Kreuzentropie zwischen $\Xg$ und $\Yg$ zu minimieren muss die 
   Gewichtsfunktion von $\Yg$ differenzierbar sein. Offensichtlich ist $\Psi$ bei min-dist nicht differenzierbar. 
   Somit betrachten wir eine stetige Approximation von $\Psi$,

   \begin{align} % TODO: Plot
      \wY(x,y) \coloneqq (1 + a\norm{x-y}_2^{2b})^{-1}, 
      \label{eq:wY}
   \end{align}

   wobei $a$ und $b$ Hyperparameter sind. Die Wahl von $a$ und $b$ kann beispielsweise mittels der Methode der kleinsten Quadrate 
   bezüglich $\Psi$ optimiert werden. 

   Die Kreuzentropie lässt sich auf gewichtete Graphen übertragen, wenn wir die Gewichte als Zugehörigkeitsgrade interpretieren,

   \begin{alignat}{2}
      C_\text{cross}(\Xg, \Yg) &\coloneqq \sum_{e \in X \times X} \wX(e) \log\left(\frac{\wX(e)}{\wY(e)}\right) + (1-\wX(e)) \log\left(\frac{1-\wX(e)}{1-\wY(e)}\right) \\
                               &= \begin{aligned}[t]
                                    & \sum_{e \in X \times X} \wX(e) \log(\wX(e)) + (1-\wX(e)) \log(1-\wX(e)) \\
                                    &- \sum_{e \in X \times X} \wX(e) \log(\wY(e)) + (1-\wX(e)) \log(1-\wY(e))
                                  \end{aligned} \\
                               &= C_\mathcal{X} - \sum_{e \in X \times X} \wX(e) \log(\wY(e)) + (1-\wX(e)) \log(1-\wY(e)) \label{eq:loss}
   \end{alignat}

   Dabei ist $C_\mathcal{X}$ von $\Yrep$ unabhängig und somit bei der Minimierung der Kreuzentropie nicht von Relevanz. 
   Eine Modifizierung des stochastischen Gradienten Verfahrens zur Minimierung von Gleichung (\ref{eq:loss}) werden wir in Abschnitt \ref{seq:SGD} betrachten.

   Die Überlegungen können wir nun im Pseudo-Code zusammenfassen. 

   \begin{algorithm}
   \caption{UMAP Algorithmus}
   \label{algorithm:umap}
   \begin{algorithmic}[1]
   \Function{UMAP}{$X, N, D, d, \code{min-dist}, \code{n-epochs}$}  %description
      \For{$\xxi \in X$}
         \State $knn(\xxi) \gets k\text{-nächste-Nachbarn}(\xxi)$ \label{alg:kNN}
         \State $graph(\xxi) \gets (\Xgi, \wXi)$ \label{alg:graph}
      \EndFor
      \State $\Xg \gets \text{gewichtete Adjazenzmatrix}(\tConorm graph(\xxi))$ \label{alg:adj}
      \State $D \gets \text{Grad-Matrix des Graphen } \Xg$ \label{alg:grad}
      \State $L \gets D^{1/2} (D-\Xg) D^{1/2}$ \Comment{Symmetrische normalisierte Laplace-Matrix}\label{alg:lapl}
      \State $evec \gets \text{sortierte Eigenvektoren von } L$ \label{alg:evec}
      \State $Y \gets evec[1,\dots,d\text{+}1]$ \label{alg:spec}
      \State $Y \gets \textsc{OptimiereEinbettung}(Y, \code{min-dist}, \code{n-epochs})$ \Comment{siehe Alg. \ref{algorithm:optimize}} \label{alg:opt}
      \State \textbf{return} $Y$
   \EndFunction
   \end{algorithmic}
   \end{algorithm}

   In Abschnitt \ref{seq:kNN} werden zwei effiziente Verfahren für die k-nächste-Nachbarn Suche (Zeile \ref{alg:kNN}) angeben. 

   Die in Zeile \ref{alg:graph} beschriebene Form die Graphen der $\xxi$ zu konstruieren scheint auf den ersten Blick sehr viel Speicher zu benötigen, 
   nämlich $\mathcal{O}(N^3)$. Dabei wird anstatt $N$ nur eine Adjazenzmatrix $A$ benötigt, in $A$ mit $a_{ij} = \wXi(\xxi,\xxj)$. Wir haben zuvor 
   die Notation mit $N$ Adjazenzmatrizen gewählt, um die Verbindung mit der Konstruktion des UMAP Verfahrens aus Kapitel \ref{UMAP} zu verdeutlichen. 
   Das betrachten von $A$ ermöglicht uns eine effiziente Implementierung der t-Conorm in Zeile \ref{alg:adj}, dabei gilt $\Xg = A + A^\top - A \circ A^\top$, 
   wobei $\circ$ das elementweise Produkt ist.

   Die Zeilen \ref{alg:grad} - \ref{alg:spec} beschreiben eine Alternative Initialisierung von $Y$ mit der spektralen Einbettung der symmetrischen Laplace-Matrix. 
   Die Grad-Matrix $D$ in Zeile \ref{alg:grad} ist eine Diagonalmatrix, wobei  
   $d_{ii} \coloneqq  \sum_{1 \leq j \leq N} \wX(\xxi, \xxj), (1 \leq i \leq N)$. 
   Für den Spezialfall, einer ungewichteten Adjazenzmatrix, beschreibt $d_{ii}$ also den Grad des Knoten $i$. Die in Zeile \ref{alg:lapl} 
   beschriebene Matrix $L$ nennt man symmetrische normalisierte Laplace-Matrix. Die Eigenwerte einer Adjazenzmatrix nennt man das \textit{Spektrum des Graphen}. 
   Dabei wählt man den Spektrale Einbettung der Laplace-Matrix, da diese eine diskrete Form des Laplace-Beltrami Operators ist \cite{Chung}. 
   Der Laplace-Beltrami Operator ist eine Erweiterung des Laplace Operators auf Riemannsche Mannigfaltigkeiten. % TODO: Überarbeiten
   Dabei ist zu beachten, dass eine Lösung des Eigenwertproblems nur von der Größe der Einbettungsdimension abhängig ist. Da wird stets $d \ll D$ 
   betrachten ist eine effiziente Implementierung mittels sukzessiver Eigenwertsuche möglich. 

   Eine Beschreibung der Optimierung in Zeile \ref{alg:opt} liefern wir in Abschnit \ref{seq:SGD}.

%----------------------------------------------------------------------------------------

\section{Profiling} \label{sec:profile}
   In Kapitel \ref{Experimente} werden wir genauer auf die tatsächliche Laufzeit des UMAP Algorithmus eingehen. 
   An dieser Stelle möchten wir die rechenintensiven Subroutinen des Verfahrens ausmachen. Dafür haben wir den 
   Python cProfiler verwendet, dieser misst die Laufzeit der aufgerufenen Funktionen. 
   Um für verschiedene Umgebungsdimensionen vergleichbare Ergebnisse zu erhalten, haben wir $N = \num{10000}$ 
   Datenpunkte in $10$ unterschiedlichen $D = [100, 500, 1000, 5000, 10000, 50000]$-dimensionalen Gauß-verteilten 
   Datenwolken gewählt. Diese Daten wurden dann in den zweidimensionalen Raum eingebettet.
   
   Dabei ist uns aufgefallen, dass besonders der k-nächste-Nachbarn-Algorithmus und die Optimierung mittels 
   stochastischem Gradienten Verfahren einen großen Teil der Laufzeit des Verfahrens beanspruchen. In Tabelle 
   \ref{table:profiling} sind die Ergebnisse der Profilierung zusammengefasst. 
   Insbesondere scheint die k-nächste-Nachbarn Suche die Laufzeit des UMAP Verfahren für 
   hochdimensionale Daten stark zu beeinflussen. 
   Wir werden beide rechenintensiven Subroutinen im folgenden betrachten und geeignete Verbesserungen diskutieren.

   \begin{table}
   \centering
   \begin{tabular}{l|ll}
   D     & Laufzeit NN-Descent & Laufzeit der Optimierung \\ \hline
   100   & 9\%                 & 75,3\%                   \\
   500   & 12\%                & 73,8\%                   \\
   1000  & 14\%                & 72,9\%                   \\
   5000  & 30,4\%              & 58\%                     \\
   10000 & 44\%                & 45,1\%                   \\
   50000 & 78,8\%              & 14,8\%                  
   \end{tabular}
   \caption{$D$ beschreibt die Größe der Umgebungsdimension. Abhängig von D haben wir die Laufzeit des UMAP Verfahrens 
            profiliert. Die zweite und dritte Spalte beziehen sich auf die relativen Laufzeiten des kNN Verfahrens und 
            der Optimierung der Einbettung.}
   \label{table:profiling}
   \end{table} 

%----------------------------------------------------------------------------------------

% N O T I Z E N
% sub-sampling bei Wortrepräsentationen sorgt für verbesserte Laufzeiten (2x - 10x)
% und Genauigkeit weniger häufig repräsentierter Wörter nimmt zu

% Negative sampling
% Noise Contrastive Estimation (NCE), Gutmann and Hyvarinen
% Ein Gutes Modell, kann Daten und Rauschen mittels logistischer Regression unterscheiden
% Benötigt samples und numerische Wahrscheinlichkeiten der Raschverteilung 
% Neg. Sampling benötigt nur samples
% Für die Wahrsscheinlichkeitsverteilung wird in der Word2Vec Version die Unigram Verteilung ^3/4 gewählt


\section{Gradientenverfahren} \label{seq:SGD}
   Um die Zielfunktion aus GLeichung (\ref{eq:loss}) zu minimieren bietet sich die Wahl eines Gradientenverfahrens an, da eine differenzierbare 
   Approximation des Zugehörigkeitsgrades (siehe Gleichung \ref{eq:wY}) gegeben ist.

   In den vergangenen Jahren gab es viele Weiterentwicklungen, insbesondere bezüglich der Konvergenzgeschwindigkeit, 
   von Gradientenverfahren. Diese werden unter anderem für das trainieren neuronaler Netzwerke bei der Backpropgation genutzt. 
   In \cite{SGDGPU} werden verschiedene Implementierungen verglichen.

   Um den Rechenaufwand im Gradientenverfahren zu verringern, wird der Gradient in zwei Summanden aufgeteilt. Dabei wird folgende Beobachtung 
   genutzt:
   Für Kanten $\{i,j\}$ mit einem hohen Zugehörigkeitsgrad $(\wX \approx 1)$ ist der Term $(1-\wX) \log(1-\wY)$ aus Gleichung (\ref{eq:loss}) 
   nahe Null, deshalb ist es sinnvoll nur den Gradienten des Terms $\wX \log(\wY)$ zu betrachten. Für Kanten mit $\wX \approx 0)$ sollte 
   hingegen der Gradient des Terms $(1-\wX) \log(1-\wY)$ betrachtet werden. 
   Für das UMAP Verfahren wird deswegen folgende Implementierung vorgeschlagen:

   \begin{algorithm}
      \caption{Optimiere die Einbettung mittels modifiziertem SGD}
      \label{algorithm:optimize}
      \begin{algorithmic}[1]
      \Function{OptimiereEinbettung}{$Y, V, W, \text{n-epochs}$}
         \State $\alpha \gets 1.0$
         \For{$e \gets 1,\dots,\text{n-epochs}$}
            \ForAll{$\{\yyi, \yyj\}$} % TODO: !! stimmt das?
               \If{$\text{Random()} \leq \wX(\yyi, \yyj)$} \label{alg:rand}
                  \State $\yyi \gets \yyi + \alpha \cdot \nabla (\log(\wY))(\yyi, \yyj)$ \label{alg:up1}
                  \For{$l \gets 1,\dots,\text{n-neg-samples}$}
                     \State $m \gets \mathcal{U}\text{nif}((0,N))$
                     \State $\yyi \gets \yyi + \alpha \cdot \gamma \cdot \nabla (\log(1-\wY))(\yyi, \mathbf{y}_m)$ \label{alg:up2}
                  \EndFor
               \EndIf
            \EndFor
         \EndFor
         \State $\alpha \gets 1.0 - e/\text{n-epochs}$
         \State \textbf{return} $Y$
      \EndFunction
      \end{algorithmic}
      \end{algorithm}

   Die in Zeile \ref{alg:rand} beschriebene Ziehung der Stichprobe dient dazu in Zeile \ref{alg:up1} keine zusätzliche Multiplikation 
   mit $\wX$ zu machen. Diese Idee entstammt \cite{LINE}. Dadurch wird der Gradient nicht durch den Wert von $\wX$ beeinflusst. % TODO: !! Besser begründen siehe 4.2.1 in LINE

   Das in jedem Durchlauf des modifizierten SGD mehrere \textit{negative samples} betrachtet werden geht auch \cite{Mikolov} zurück. 
   Im wesentlichen verhindert dies, das sich die Vektoren der niedrigdimensionalen Darstellung häufen. Die dabei $(0,N)$-gleichverteilt 
   gezogene Stichprobe ist eine Modifizierung von \cite{Tang}. Die Wahl des Hyperparameter n-neg-samples soll laut \cite{Mikolov} zwischen zwei und $20$ liegen.  % TODO: besser begründen.

   Der Gradient in Zeile \ref{alg:up1} ist gegeben durch: 

   \begin{equation}
      \nabla (\log(\wY))(\yyi, \yyj) = \frac{-2 a b \norm{\mathbf{y}_i - \mathbf{y}_j}^{2(b-1)}_2 }{(1+\norm{\mathbf{y}_i - \mathbf{y}_j}^{2}_2)} (\mathbf{y}_i - \mathbf{y}_j)
   \end{equation}

   und der Gradient in Zeile \ref{alg:up2} durch: 
 
   \begin{equation}
      \nabla (1-\log(\wY))(\yyi, \yyj) = \frac{2 b}{(\epsilon + \norm{\mathbf{y}_i - \mathbf{y}_j}^{2}_2)(1 + a\norm{\mathbf{y}_i - \mathbf{y}_j}^{2b}_2)} (\mathbf{y}_i - \mathbf{y}_j),
   \end{equation}

   wobei der $\epsilon$-Parameter eine Division mit Null vermeidet.




   % Das UMAP Verfahren nutzt für die Optimierung der Zielfunktion die \textit{negative sampling} Methode aus dem Bereich 
   % der Worteinbettungen \cite{Mikolov}. Wir werden diese Methode vorstellen und begründen warum sich die Argumentation aus dem 
   % Bereich der Worteinbettungen auf die Dimensionsreduktion übertragen lässt.
   % Dann vervollständigen Algorithmus \ref{algorithm:umap} um die Subroutine \textsc{OptimiereEinbettung}.
   
   % % Optimiert Suchrichtung des Gradienten da wichtige Beispiele betrachtet werden. 
   % \subsection*{Negative Sampling}
   %    Die erste uns bekannte Quelle des negative sampling ist Mikolov et. al \cite{Mikolov}. Das Methode wird 
   %    für die Problemstellung entwickelt um für ein Paar $(w, c)$ eines Wortes $w$ und eines dazugehörigen Kontextes $c$ 
   %    eine Vektorrepräsentation zu finden. Dafür ist uns eine Sprache $W$ und eine Kontextmenge $C$ gegeben, wobei für 
   %    $w \in W$ die Menge $C(w)$ alle zu w passenden Kontexte angibt. Eine sinnvolle Repräsentation der Worte \enquote{Numerik}, 
   %    \enquote{Garcke}, \enquote{Algebraische Topologie}, \enquote{Scholze} wäre beispielsweise 
   %    $vec(\text{Algebraische Topologie}) - vec(\text{Scholze}) + vec(\text{Numerik}) = vec(\text{Garcke})$

   % \subsection*{Optimierung des UMAP Gradienten}
   %    Damit können wir Algorithmus \ref{algorithm:umap} vervollständigen.

   %    \begin{algorithm}
   %    \caption{OptimiereEinbettung}
   %    \label{algorithm:optimize}
   %    \begin{algorithmic}[1]
   %    \Function{OptimiereEinbettung}{$Y, V, W, \text{n-epochs}$}
   %       \State $\alpha \gets 1.0$
   %       \For{$e \gets 1,\dots,\text{n-epochs}$}
   %          \State $x \gets \dots$ % TODO: ergänze
   %       \EndFor
   %       \State $\alpha \gets 1.0 - e/\text{n-epochs}$
   %       \State \textbf{return} $Y$
   %    \EndFunction
   %    \end{algorithmic}
   %    \end{algorithm}

   %    In den uns bekannten Implementierungen wird der Gradient in Zeile REF?? außerhalb des Intervalls 
   %    $[-4,4]$ auf $-4$ bzw. $4$ gesetzt, um die Auswirkung von schlechten Kanten zu reduzieren \cite{Goodfellow}. % TODO: Referenz


   %    Für dünn-besetzte Datensätze $X$ könnte man beispielsweise das AdaGrad Verfahren betrachten, 
   %    dieses zeigte in der Praxis sehr gute Ergebnisse für dünn-besetzte Daten. % TODO: Quellen

%----------------------------------------------------------------------------------------

\section{Nächste-Nachbarn-Klassifikation} \label{seq:kNN}
   Zum effizienten finden der 1-Simplizes der topologischen Repräsentation unserer Daten, benötigen wir einen 
   k-nächste-Nachbarn-Algorithmus (kurz: \textit{kNN-Algorithmus}). 
   %TODO: Beschreibung was ein kNN-Alg macht

   Das Ergebnis eines kNN-Algorithmus wird meist in einem ungerichteten Graph -- dem kNN-Graph -- dargestellt, 
   wobei die Knoten den Datenpunkten entsprechen und die Kanten den Nachbarschaftsbeziehungen, 
   somit besitzt jeder Knoten Grad k.

   Bei einer naiven Implementierung beträgt die Laufzeit $\mathcal{O}(N^2D)$ (wobei N die Anzahl der Datenpunkte  % TODO: warum N^2D?
   und D die Dimension der Datenpunkte ist). Mit einer effizienten Implementierung ist in der Praxis eine 
   annähernd in N lineare Laufzeit möglich. Die Herangehensweisen lassen sich nach \cite{Tang} in drei Kategorien 
   einteilen. (1) Baum basierte Verfahren auf Partitionen des Raumes, (2) Hashfunktionen auf lokalen Teilgebieten des Raumes 
   (3) Nachbarschafts-Erkundungen. 

   Wir möchten nun zwei Verfahren vorstellen. 

   %-----------------------------------

   \subsection*{NN-Descent}
      Der NN-Descent Algorithmus \cite{k-NNG} beruht auf dem Prinzip der Nachbarschafts-Erkundungen. Dabei wird ein 
      initialer kNN-Graph iterativ verbessert, unter der Annahme, dass die Nachbarschaftsbeziehung 
      transitiv ist, für zwei vorhandene Nachbarschaftspaare $(x, y), (y,z)$ also mit hoher Wahrscheinlichkeit auch 
      ein Nachbarschaftspaar $(x,z)$ im kNN-Graph existiert. 
      Der initiale Graph im NN-Descent Verfahren wird dabei zufällig gewählt. Dies kann dazu führen, dass nur 
      lokal optimale k-NN-Graphen gefunden werden. Dies könnte laut \cite{EFANNA} dadurch verbessert werden, indem 
      für die Initialisierung \enquote{random projection trees}, wie in \cite{Tang}, verwendet werden. 

      Ein Vorteil des NN-Descent Verfahren ist, dass kein globaler Index der verwaltet werden muss. Somit ist eine 
      Anwendung auf großen Datensätzen möglich welche nicht komplett in den Arbeitsspeicher (RAM) des verwendeten Rechners 
      geladen werden können. 

      Nachteil des NN-Descent Algorithmus ist die Speicherplatzkomplexität, diese ist durch $\mathcal{O}(N^2)$ beschränkt. 
      Im wesentlichen ist dies dadurch begründet, dass paarweise die Ähnlichkeit, welche im Falle von UMAP durch die Metrik 
      des Umgebungsraums gegeben ist, gespeichert wird. Aufgrund dessen, dass nur lokale Optima garantiert sind, ist das Ergebnis 
      des NN-Descent Verfahren approximativ. In \cite{UMAP} wird jedoch argumentiert, dass dies wegen des Informationsverlust 
      bei Dimensionsreduktionen kaum Auswirkungen auf die resultierende Einbettung hat. 

   %-----------------------------------

   \subsection*{FAISS}
      Das FAISS Bibliothek \cite{FAISS} nutzt die Architektur einer GPU aus. Dabei baut FAISS eine effiziente Datenstruktur, 
      welche für die Vektoren die nächsten Nachbarn speichert. Somit ist eine sehr schnelle Implementierung 
      für das aufstellen des k-NN-Graphen möglich. 

      Der RAM der meisten GPUs ist stark begrenzt. Um dennoch mit großen Datensätzen zu arbeiten werden komprimierte 
      Darstellungen der Vektoren genutzt. % TODO: Referenzen einfügen, aus FAISS
      Für FAISS werden \enquote{product quantization codes} genutzt. % TODO: referenz und kurze Beschreibung 

      Vorteile der FAISS Datenstruktur sind die effiziente Implementierung auf GPUs und das sowohl exakte Ergebnisse 
      sowie Approximationen für die nächsten Nachbarn angegeben werden können. Die Rückgabe approximativer 
      Ergebnisse erhält Laufzeit sowie Speicherplatz Vorteile. 

      Nachteil des FAISS Verfahren ist, dass zurzeit nur die euklidische Distanz unterstützt wird. 

%----------------------------------------------------------------------------------------

% TODO: Complete this.
\section{Hyperparameter} \label{seq:hyper}

   \begin{itemize}
      \item \code{n\_neighbors}:
            beschreibender Text
        % The size of local neighborhood (in terms of number of neighboring
        % sample points) used for manifold approximation. Larger values
        % result in more global views of the manifold, while smaller
        % values result in more local data being preserved. In general
        % values should be in the range 2 to 100.
      \item $\code{metric}$
         Text
      \item $\code{n-epochs}$
         Text
      \item \code{set\_op\_mix\_ratio}
        % Interpolate between (fuzzy) union and intersection as the set operation
        % used to combine local fuzzy simplicial sets to obtain a global fuzzy
        % simplicial sets. Both fuzzy set operations use the product t-norm.
        % The value of this parameter should be between 0.0 and 1.0; a value of
        % 1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy
        % intersection.
      \item $\code{local\_connectivity}$ 
         % The local connectivity required -- i.e. the number of nearest
         % neighbors that should be assumed to be connected at a local level.
         % The higher this value the more connected the manifold becomes
         % locally. In practice this should be not more than the local intrinsic
         % dimension of the manifold.
      \item \code{repulsion\_strength}
         % Weighting applied to negative samples in low dimensional embedding
         % optimization. Values higher than one will result in greater weight
         % being given to negative samples.
      \item \code{a, b}:
            Erwähne wie a und b standardmäßig gewählt werden, zeige Plot von a, b
            Erwähne \code{min-dist} und \code{spread}
      \item \code{negative\_sample\_weight}
   \end{itemize}

%----------------------------------------------------------------------------------------      
